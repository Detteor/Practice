{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspections and Enforcement"
   ]
  },
  {
   "source": [
    "<b> TO DO: </b>\n",
    "<ul>\n",
    "<li><del>Create Year Column on PROD.gisadmin.Violations Table.</del></li>\n",
    "<li>Use dask to create a lot of DFs for processing.(Long Term)</li>\n",
    "<li><del>Add GeoSpatial Data to Violations for Historical Violations.</del></li>\n",
    "<li><del>Remove Violations from the DB and upload the new Violations from MGO (when available) and AS400 Data.</del></li>\n",
    "<li>Create SQL job to add Violations to esridb.</li>\n",
    "<li>Test what happens when two violations in the same year. (Long Term)</li>\n",
    "<li>Add to Daily Script.</li>\n",
    "</ul>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use arcNew environment with this notebook.\n",
    "#Current Location: C:\\ProgramData\\Anaconda3\\envs\\arcNew\\python.exe\n",
    "import arcpy as arc\n",
    "import os\n",
    "from shutil import copy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arcgis import features\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "# import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "now = dt.now()\n",
    "mStr = now.strftime('%m%Y')\n",
    "dStr = now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'\n",
    "# print(netDB)"
   ]
  },
  {
   "source": [
    "### Create Folders for Violations Review Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Folders for Permits Data\n",
    "vFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "vProcessing = vFolder / 'Violations' / f'{dStr}'\n",
    "if vProcessing.exists() == True:\n",
    "    print(f'{vProcessing} already exist.')\n",
    "else:\n",
    "    vProcessing.mkdir(parents=True)\n",
    "    print(f'Created {vProcessing}.')\n",
    "\n",
    "vFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "vReview = vFR / 'Violations' / f'{dStr}'\n",
    "if vReview.exists() == True:\n",
    "    print(f'{vReview} already exist')\n",
    "else:\n",
    "    vReview.mkdir(parents=True)\n",
    "    print(f'Created {vReview}')"
   ]
  },
  {
   "source": [
    "### Create New File GeoDatabase and corresponding Feature Datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'PROD-WA.sde'\n",
    "sr = arc.Describe(f'{iE / \"PROD.GISADMIN.SheriffSales\"}').spatialReference\n",
    "outGDB = gisPath / vFolder / f'Data_{mStr}.gdb'\n",
    "# dsA = gisPath / pFolder / f'{outGDB}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dStr}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{vFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "source": [
    "### Create Violations Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<b>Does not work with Parcel Fabric due to bug with 10.5.1</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbViol = 'PROD.gisadmin.Violations'\n",
    "# dbViolD = 'PROD.gisadmin.violations_description'\n",
    "dbAdd = 'PROD.ADDRESSINGADMIN.Addressing'\n",
    "dbSufP = 'PROD.GISADMIN.Suffix_Parcels'\n",
    "dbMPTab = 'PROD.gisadmin.MGOPermits'\n",
    "dbAPTab = 'PROD.gisadmin.All_Permits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create paths to the data instead of using env.workspace makes it so that I can use arc Walk\n",
    "arc.env.workspace = f'{iE}'\n",
    "\n",
    "fcList = [dbAdd, dbSufP]\n",
    "tList = [dbMPTab, dbAPTab, dbViol]\n",
    "cParcels = []\n",
    "cTables = []\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in fcList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}'\n",
    "            if arc.Exists(f'{locGDB / b}'):\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.FeatureClassToFeatureClass_conversion(f, f'{locGDB}', f'{b}')\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in tList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}' #Tables need to have a date string since they won't be under the dataset\n",
    "            # c = locGDB.parent\n",
    "            if arc.Exists(f'{outGDB / b}'):\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.TableToTable_conversion(f, f'{outGDB}', f'{b}')\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "# print(cParcels)\n",
    "# print(cTables)"
   ]
  },
  {
   "source": [
    "### Copy Parcel Fabric to local folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently using env.workspace to move parcelfabric needs to be changed to use path\n",
    "dbWS = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "arc.env.workspace = f'{dbWS}'\n",
    "pfParcels = 'PROD.MAPPINGADMIN.ParcelFabric_Parcels'\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "a = pfParcels.split('.')[-1]\n",
    "b = f'{a}_{dStr}'\n",
    "# print(f'{iEPath[1]}')\n",
    "if arc.Exists(f'{locGDB / b}'):\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'{b} already exists')\n",
    "else:\n",
    "    print(f'Copying Parcel Fabric to {locGDB}')\n",
    "    #print(a)\n",
    "    # g = f'{f}'\n",
    "    #print(g)\n",
    "    arc.FeatureClassToFeatureClass_conversion(pfParcels, f'{locGDB}', b, where_clause=exp)\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "source": [
    "Create DataFrame for processing from Parcel Fabric Points"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Points out of polygons in ParcelFabric\n",
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "# print(pfFC)\n",
    "pfPoints = locGDB / f'pfPoints_{dStr}'\n",
    "# print(pfPoints)\n",
    "if arc.Exists(f'{pfPoints}'):\n",
    "    print(f'{pfPoints.name} already exists.')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(pfFC), f'{pfPoints}', \"INSIDE\")\n",
    "    print(f'{pfPoints.name} has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfFields = [f.name for f in arc.ListFields(pfFC)]\n",
    "# print(pfFields)\n",
    "pf_df = features.GeoAccessor.from_featureclass(pfPoints, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')\n",
    "# pf_df.shape"
   ]
  },
  {
   "source": [
    "Create DataFrame from Parcel Fabric Polygons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "ppf_df = features.GeoAccessor.from_featureclass(pfFC, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Violations Table DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy table to the locGDB from ESRIDB\n",
    "vTab = [f for f in cTables if f.name.startswith('Violations') == True][0]\n",
    "\n",
    "v_df = features.GeoAccessor.from_table(f'{vTab}', skip_nulls=False)\n",
    "v_df.drop(columns='OBJECTID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newyear(field):\n",
    "    if int(field) > 80 and int(field) < 100:\n",
    "        return \"19\" + str(field)\n",
    "    elif int(field) >= 0 and int(field) <=9:\n",
    "        return \"200\" + str(field)\n",
    "    elif int(field) > 10 and int(field) < 80:\n",
    "        return \"20\" + str(field)"
   ]
  },
  {
   "source": [
    "Apply newyear function and sort by Year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df.YEAR = v_df.YEAR.apply(newyear)"
   ]
  },
  {
   "source": [
    "Merge with Parcel Fabric Points to obtain GeoSpatial Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vLoc = locGDB / f'Violation_{dStr}'\n",
    "df_gf = pf_df.merge(v_df, how='outer', left_on='Name', right_on='PARCELID', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(f'{gisPath / \"Corrupted.csv\"}')\n",
    "df_gf = df_gf.loc[df_gf._merge != 'left_only'].copy(deep=True)\n",
    "df_gf.drop(columns='Name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "source": [
    "Merge Violation Suffix Parcels with Parcels with Suffix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ParcelsWithSuffix and create DataFrame\n",
    "psLoc = locGDB / f'ParcelWithSuffix_{dStr}'\n",
    "# psLoc = locGDB.parent / 'Daily_05_04' / 'ParcelWithSuffix'\n",
    "psDF = features.GeoAccessor.from_featureclass(psLoc, fields=['Name'])\n",
    "\n",
    "ps_df = df_gf[(df_gf._merge == 'right_only') & (~df_gf.PARCELID.str.contains('-00001', na=False))].copy(deep=True)\n",
    "ps_df.drop(columns=['_merge','SHAPE'], inplace=True)\n",
    "\n",
    "m_ps = psDF.merge(ps_df, how='outer', left_on='Name', right_on='PARCELID', indicator=True)"
   ]
  },
  {
   "source": [
    "Export the Parcel #s that are not matching and the ones that did match (these have limited geospatial information)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mDM = vReview / 'PS_NF.csv' #NF = Not Found\n",
    "m_ps[m_ps._merge == 'right_only'].to_csv(mDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new DF with matching Parcel #s \n",
    "eMP = m_ps[m_ps._merge != 'left_only'].copy(deep=True)\n",
    "eMP.drop(columns=['Name','_merge'], inplace=True)"
   ]
  },
  {
   "source": [
    "Export to csv the Parcel #s that don't have a match in Parcel Fabric and remove them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pNF = vReview / 'Parcel_NF.csv' #NF = NOT FOUND\n",
    "nf_df = df_gf[(df_gf._merge == 'right_only') & (df_gf.PARCELID.str.contains('-00001'))].copy(deep=True)\n",
    "nf_df.to_csv(pNF, index=False)\n",
    "nf_df.drop(columns='_merge', inplace=True)\n",
    "\n",
    "#Create new DF without suffix parcels and the NF parcels, will merge this back in later\n",
    "ns_df = df_gf[(df_gf._merge != 'left_only') & (df_gf.PARCELID.str.contains('-00001'))].copy(deep=True)\n",
    "ns_df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "source": [
    "Combine the violations with suffix and the violations table to create Historical Violations Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_df = pd.concat([ns_df,eMP], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_df.rename(columns={'PARCELID':'NAME'}, inplace=True)"
   ]
  },
  {
   "source": [
    "Export the DataFrame to locGDB as a feature class. Contains the Historical Violations Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see why there are null years\n",
    "nYcsv = vReview / 'Viol_Null.csv'\n",
    "hv_df[hv_df.YEAR == ' '].to_csv(nYcsv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_df.SHAPE.fillna(0, inplace=True)\n",
    "hv_df.YEAR.fillna(np.nan, inplace=True)\n",
    "hv_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsr = {'x': None, 'y': None, 'spatialReference':{'wkid':26957}}\n",
    "# fr = {'spatialReference':{'wkid':26957}}\n",
    "hv_df.SHAPE = hv_df.SHAPE.apply(lambda x: fsr if x == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vCurrent = locGDB / f'HViolation_{dStr}'\n",
    "# geo_df = gpd.GeoDataFrame(hv_df, geometry='SHAPE', crs=26957)\n",
    "# vSTest = locGDB / f'HViol_{dStr}'\n",
    "# vSTest = gisPath / 'lol.shp'\n",
    "# geo_df.to_file(f'{vSTest}')\n",
    "# gg = features.GeoAccessor.from_geodataframe(geo_df)\n",
    "# hv_df.spatial.to_featureclass(f'{vSTest}', sanitize_columns=False)\n",
    "hv_df.spatial.to_featureclass(f'{vCurrent}', sanitize_columns=False)\n",
    "print(f'Exported {vCurrent.name}')"
   ]
  },
  {
   "source": [
    "#### Create Current Violations by removing duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Export Null Dates to CSV for review"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create copy just in case of corruption. Sort by year to make sure that you have the most recent violation\n",
    "dup_v = hv_df.copy(deep=True)\n",
    "dup_v = dup_v.sort_values(by='YEAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_v = dup_v.loc[dup_v.YEAR.notnull()].copy(deep=True)"
   ]
  },
  {
   "source": [
    " Remove duplicates based on the most recent year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_v.drop_duplicates(subset=['NAME'], keep='last', ignore_index=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV for review and drop shape column to add as table to local GDB\n",
    "dupCsv = vReview / 'Violations_Current.csv'\n",
    "dup_v.to_csv(dupCsv, index=False)\n",
    "# tabV = dup_v.drop(columns='SHAPE')\n",
    "# tabV.rename(columns={'NAME':'Name'}, inplace=True)\n",
    "\n",
    "#export to local GDB as table\n",
    "# tabName = locGDB / f'Viol_{dStr}'\n",
    "# tabV.spatial.to_table(str(tabName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop YEAR column to match the Feature Class on esridb\n",
    "dup_v.drop(columns='YEAR', inplace=True)\n",
    "dup_v.rename(columns={'NAME':'Name'}, inplace=True)\n",
    "print(f\"Dropped YEAR column from Duplicate DF\")"
   ]
  },
  {
   "source": [
    "Export Feature class to locGDB to be used with REST Server"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outDup = locGDB / f'Violation_{dStr}'\n",
    "dup_v.spatial.to_featureclass(f'{outDup}', sanitize_columns=False)\n",
    "print(f'Copied {outDup.name} to {locGDB.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbViol = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.InspectionandEnforcement\" / \"PROD.GISADMIN.Violation\"\n",
    "arc.TruncateTable_management(str(dbViol))\n",
    "arc.Append_management(str(outDup), str(dbViol), schema_type='NO_TEST')\n",
    "print(f'{dbViol.name} Updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd075dad33b15f83a112a28040a32ba03061e24a62911504b8f236300170e604621",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}