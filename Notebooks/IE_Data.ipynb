{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspections and Enforcement"
   ]
  },
  {
   "source": [
    "<b> TO DO: </b>\n",
    "<ul> <li>get values out of the year column to create multiple DFs using those years.</li>\n",
    "<li>Create Year Column on PROD.gisadmin.Violations Table.</li>\n",
    "<li>also can create a dictionary with those years in there.</li>\n",
    "<li>use dask to create a lot of DFs for processing.</li>\n",
    "<li>Add GeoSpatial Data to All Permits for Historical Permits.</li>\n",
    "<li>Remove Permits from the DB and upload the new Permits from MGO and AS400 Data.</li>\n",
    "</ul>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use arcNew environment with this notebook.\n",
    "#Current Location: C:\\ProgramData\\Anaconda3\\envs\\arcNew\\python.exe\n",
    "import arcpy as arc\n",
    "import os\n",
    "from shutil import copy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arcgis import features\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path\n",
    "# import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "now = dt.now()\n",
    "mStr = now.strftime('%m%Y')\n",
    "dStr = now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'\n",
    "# print(netDB)"
   ]
  },
  {
   "source": [
    "### Create Folders for Violations Review Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Folders for Permits Data\n",
    "vFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "vProcessing = vFolder / 'Violations' / f'{dStr}'\n",
    "if vProcessing.exists() == True:\n",
    "    print(f'{vProcessing} already exist.')\n",
    "else:\n",
    "    vProcessing.mkdir(parents=True)\n",
    "    print(f'Created {vProcessing}.')\n",
    "\n",
    "vFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "vReview = vFR / 'Violations' / f'{dStr}'\n",
    "if vReview.exists() == True:\n",
    "    print(f'{vReview} already exist')\n",
    "else:\n",
    "    vReview.mkdir(parents=True)\n",
    "    print(f'Created {vReview}')"
   ]
  },
  {
   "source": [
    "### Create New File GeoDatabase and corresponding Feature Datasets."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'PROD-WA.sde'\n",
    "sr = arc.Describe(f'{iE / \"PROD.GISADMIN.SheriffSales\"}').spatialReference\n",
    "outGDB = gisPath / vFolder / f'Data_{mStr}.gdb'\n",
    "# dsA = gisPath / pFolder / f'{outGDB}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dStr}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{vFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "source": [
    "### Create Violations Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<b>Does not work with Parcel Fabric due to bug with 10.5.1</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbViol = 'PROD.gisadmin.Violations'\n",
    "# dbViolD = 'PROD.gisadmin.violations_description'\n",
    "dbAdd = 'PROD.ADDRESSINGADMIN.Addressing'\n",
    "dbSufP = 'PROD.GISADMIN.Suffix_Parcels'\n",
    "dbMPTab = 'PROD.gisadmin.MGOPermits'\n",
    "dbAPTab = 'PROD.gisadmin.All_Permits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create paths to the data instead of using env.workspace makes it so that I can use arc Walk\n",
    "arc.env.workspace = f'{iE}'\n",
    "\n",
    "fcList = [dbAdd, dbSufP]\n",
    "tList = [dbMPTab, dbAPTab, dbViol]\n",
    "cParcels = []\n",
    "cTables = []\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in fcList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}'\n",
    "            if arc.Exists(f'{locGDB / b}'):\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.FeatureClassToFeatureClass_conversion(f, f'{locGDB}', f'{b}')\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in tList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}' #Tables need to have a date string since they won't be under the dataset\n",
    "            # c = locGDB.parent\n",
    "            if arc.Exists(f'{outGDB / b}'):\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.TableToTable_conversion(f, f'{outGDB}', f'{b}')\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "# print(cParcels)\n",
    "# print(cTables)"
   ]
  },
  {
   "source": [
    "### Copy Parcel Fabric to local folder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently using env.workspace to move parcelfabric needs to be changed to use path\n",
    "dbWS = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "arc.env.workspace = f'{dbWS}'\n",
    "pfParcels = 'PROD.MAPPINGADMIN.ParcelFabric_Parcels'\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "a = pfParcels.split('.')[-1]\n",
    "b = f'{a}_{dStr}'\n",
    "# print(f'{iEPath[1]}')\n",
    "if arc.Exists(f'{locGDB / b}'):\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'{b} already exists')\n",
    "else:\n",
    "    print(f'Copying Parcel Fabric to {locGDB}')\n",
    "    #print(a)\n",
    "    # g = f'{f}'\n",
    "    #print(g)\n",
    "    arc.FeatureClassToFeatureClass_conversion(pfParcels, f'{locGDB}', b, where_clause=exp)\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "source": [
    "Create DataFrame for processing from Parcel Fabric Points"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Points out of polygons in ParcelFabric\n",
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "# print(pfFC)\n",
    "pfPoints = locGDB / f'pfPoints_{dStr}'\n",
    "# print(pfPoints)\n",
    "if arc.Exists(f'{pfPoints}'):\n",
    "    print(f'{pfPoints.name} already exists.')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(pfFC), f'{pfPoints}', \"INSIDE\")\n",
    "    print(f'{pfPoints.name} has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfFields = [f.name for f in arc.ListFields(pfFC)]\n",
    "# print(pfFields)\n",
    "pf_df = features.GeoAccessor.from_featureclass(pfPoints, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')\n",
    "# pf_df.shape"
   ]
  },
  {
   "source": [
    "Create DataFrame from Parcel Fabric Polygons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "ppf_df = features.GeoAccessor.from_featureclass(pfFC, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Violations Table DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy table to the locGDB from ESRIDB\n",
    "vTab = [f for f in cTables if f.name.startswith('Violations') == True][0]\n",
    "\n",
    "v_df = features.GeoAccessor.from_table(f'{vTab}', skip_nulls=False)\n",
    "v_df.drop(columns='OBJECTID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newyear(field):\n",
    "    if int(field) > 80 and int(field) < 100:\n",
    "        return \"19\" + str(field)\n",
    "    elif int(field) >= 0 and int(field) <=9:\n",
    "        return \"200\" + str(field)\n",
    "    elif int(field) > 10 and int(field) < 80:\n",
    "        return \"20\" + str(field)"
   ]
  },
  {
   "source": [
    "Apply newyear function and sort by Year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df.YEAR = v_df.YEAR.apply(newyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_df.columns"
   ]
  },
  {
   "source": [
    "Merge with Parcel Fabric Points to obtain GeoSpatial Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vLoc = locGDB / f'Violation_{dStr}'\n",
    "df_gf = pf_df.merge(v_df, how='outer', left_on='Name', right_on='PARCELID', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(f'{gisPath / \"Corrupted.csv\"}')\n",
    "df_gf = df_gf.loc[df_gf._merge != 'left_only'].copy(deep=True)\n",
    "df_gf.drop(columns='Name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gf.Name = df_gf.Name.astype(str)\n",
    "# df_gf.VIOLATIONSTATUS = df_gf.VIOLATIONSTATUS.astype(str)\n",
    "# df_gf.VIOLATIONTYPE = df_gf.VIOLATIONTYPE.astype(str)\n",
    "# df_gf.VIOLATIONDESC = df_gf.VIOLATIONDESC.astype(str)\n",
    "# df_gf.YEAR = df_gf.YEAR.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fV = df_gf.spatial.to_featureset()\n",
    "# fV.fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fV.spatial_reference\n",
    "# a = fV.features\n",
    "# f = []\n",
    "# for feat in fV.features:\n",
    "#     a = feat.geometry\n",
    "#     f.append(a)\n",
    "# l = gisPath / 'op.txt'\n",
    "# l.touch()\n",
    "# g = l.open('a', newline='\\n')\n",
    "# for x in f:\n",
    "#     g.write(str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fV.save(f'{outGDB}', 'ViolTT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_gf.drop(columns='SHAPE', inplace=True)\n",
    "# df_gf.spatial.to_table(f'{outGDB / \"VTest\"}')"
   ]
  },
  {
   "source": [
    "Merge Violation Suffix Parcels with Parcels with Suffix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gf.columns"
   ]
  },
  {
   "source": [
    "Merge Parcel With Suffix with Violations with Suffix."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import ParcelsWithSuffix and create DataFrame\n",
    "psLoc = locGDB / f'ParcelWithSuffix_{dStr}'\n",
    "# psLoc = locGDB.parent / 'Daily_05_04' / 'ParcelWithSuffix'\n",
    "psDF = features.GeoAccessor.from_featureclass(psLoc, fields=['Name'])\n",
    "\n",
    "ps_df = df_gf[(df_gf._merge == 'right_only') & (~df_gf.PARCELID.str.contains('-00001', na=False))].copy(deep=True)\n",
    "ps_df.drop(columns=['_merge','SHAPE'], inplace=True)\n",
    "\n",
    "m_ps = psDF.merge(ps_df, how='outer', left_on='Name', right_on='PARCELID', indicator=True)"
   ]
  },
  {
   "source": [
    "Export the Parcel #s that don't match with the Parcel With Suffix Layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mDM = vReview / 'PS_NF.csv' #NF = Not Found\n",
    "m_ps[m_ps._merge == 'right_only'].to_csv(mDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create new DF with matching Parcel #s \n",
    "eMP = m_ps[m_ps._merge != 'left_only'].copy(deep=True)\n",
    "eMP.drop(columns=['Name','_merge'], inplace=True)"
   ]
  },
  {
   "source": [
    "Export to csv the Parcel #s that don't have a match in Parcel Fabric and remove them"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pNF = vReview / 'Parcel_NF.csv' #NF = NOT FOUND\n",
    "nf_df = df_gf[(df_gf._merge == 'right_only') & (df_gf.PARCELID.str.contains('-00001'))].copy(deep=True)\n",
    "nf_df.to_csv(pNF, index=False)\n",
    "nf_df.drop(columns='_merge', inplace=True)\n",
    "\n",
    "#Create new DF without suffix parcels and the NF parcels, will merge this back in later\n",
    "ns_df = df_gf[(df_gf._merge != 'left_only') & (df_gf.PARCELID.str.contains('-00001'))].copy(deep=True)\n",
    "ns_df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eMP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ns_df.columns)\n",
    "print(eMP.columns)"
   ]
  },
  {
   "source": [
    "Combine the violations with suffix and the violations table to create Historical Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_df = pd.concat([ns_df,eMP], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_df.rename(columns={'PARCELID':'NAME'}, inplace=True)"
   ]
  },
  {
   "source": [
    "Export the DataFrame to locGDB as a feature class. Contains the Historical Violations Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check to see why there are null years\n",
    "nYcsv = vReview / 'Viol_Null.csv'\n",
    "hv_df[hv_df.YEAR == ' '].to_csv(nYcsv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_df.SHAPE.fillna(np.nan, inplace=True)\n",
    "hv_df.YEAR.fillna(np.nan, inplace=True)\n",
    "hv_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsr = {'x': None, 'y': None, 'spatialReference':{'wkid':26957}}\n",
    "# fr = {'spatialReference':{'wkid':26957}}\n",
    "hv_df.SHAPE = hv_df.SHAPE.apply(lambda x: fsr if x == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vCurrent = locGDB / f'HViolation_{dStr}'\n",
    "# geo_df = gpd.GeoDataFrame(hv_df, geometry='SHAPE', crs=26957)\n",
    "# vSTest = locGDB / f'HViol_{dStr}'\n",
    "# vSTest = gisPath / 'lol.shp'\n",
    "# geo_df.to_file(f'{vSTest}')\n",
    "# gg = features.GeoAccessor.from_geodataframe(geo_df)\n",
    "# hv_df.spatial.to_featureclass(f'{vSTest}', sanitize_columns=False)\n",
    "hv_df.spatial.to_featureclass(f'{vCurrent}', sanitize_columns=False)"
   ]
  },
  {
   "source": [
    "#### Create Current Violations by removing duplicates"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Export Null Dates to CSV for review"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dup_v = hv_df.copy(deep=True)\n",
    "dup_v = dup_v.sort_values(by='YEAR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_v = dup_v.loc[dup_v.YEAR.notnull()]"
   ]
  },
  {
   "source": [
    " Remove duplicates based on the most recent year."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_v.drop_duplicates(subset=['PARCELID'], keep='last', ignore_index=True, inplace=True)"
   ]
  },
  {
   "source": [
    "Export Feature class to locGDB to be used with REST Server"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd075dad33b15f83a112a28040a32ba03061e24a62911504b8f236300170e604621",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}