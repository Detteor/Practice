{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Permits, Violations, and TaxParcels File with MGO Data and AS 400 Data.\n",
    "\n"
   ]
  },
  {
   "source": [
    "<b> TO DO: </b>\n",
    "<ul> <li>get values out of the year column to create multiple DFs using those years.</li>\n",
    "<li>also can create a dictionary with those years in there.</li>\n",
    "<li>use dask to create a lot of DFs for processing.</li>\n",
    "<li>Add GeoSpatial Data to All Permits for Historical Permits.</li>\n",
    "<li>Remove Permits from the DB and upload the new Permits from MGO and AS400 Data.</li>\n",
    "</ul>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Declare Global Variables for geoprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use arcNew environment with this notebook.\n",
    "#Current Location: C:\\ProgramData\\Anaconda3\\envs\\arcNew\\python.exe\n",
    "import arcpy as arc\n",
    "import os\n",
    "from shutil import copy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arcgis import features\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "now = dt.now()\n",
    "mStr = now.strftime('%m%Y')\n",
    "dStr = now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'\n",
    "# print(netDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Only use with notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For use when in production."
   ]
  },
  {
   "source": [
    "### Create Folders for Permits Review Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Folders for Permits Data\n",
    "pFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "pProcessing = pFolder / 'Permits' / f'{dStr}'\n",
    "if pProcessing.exists() == True:\n",
    "    print(f'{pProcessing} already exist.')\n",
    "else:\n",
    "    pProcessing.mkdir(parents=True)\n",
    "    print(f'Created {pProcessing}.')\n",
    "\n",
    "pFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "pReview = pFR / 'Permits' / f'{dStr}'\n",
    "if pReview.exists() == True:\n",
    "    print(f'{pReview} already exist')\n",
    "else:\n",
    "    pReview.mkdir(parents=True)\n",
    "    print(f'Created {pReview}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New File GeoDatabase and corresponding Feature Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "sr = arc.Describe(f'{iE}').spatialReference\n",
    "outGDB = gisPath / pFolder / f'Data_{mStr}.gdb'\n",
    "# dsA = gisPath / pFolder / f'{outGDB}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dStr}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{pFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parcel Fabric DataFrame and Points FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Does not work with Parcel Fabric due to bug with 10.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbAdd = 'PROD.ADDRESSINGADMIN.Addressing'\n",
    "dbSufP = 'PROD.GISADMIN.Suffix_Parcels'\n",
    "dbMPTab = 'PROD.gisadmin.MGOPermits'\n",
    "dbAPTab = 'PROD.gisadmin.All_Permits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create paths to the data instead of using env.workspace makes it so that I can use arc Walk\n",
    "arc.env.workspace = f'{iE.parent}'\n",
    "\n",
    "fcList = [dbAdd, dbSufP]\n",
    "tList = [dbMPTab, dbAPTab]\n",
    "cParcels = []\n",
    "cTables = []\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in fcList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}'\n",
    "            if arc.Exists(f'{locGDB / b}'):\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.FeatureClassToFeatureClass_conversion(f, f'{locGDB}', f'{b}')\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in tList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}' #Tables need to have a date string since they won't be under the dataset\n",
    "            # c = locGDB.parent\n",
    "            if arc.Exists(f'{outGDB / b}'):\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.TableToTable_conversion(f, f'{outGDB}', f'{b}')\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "# print(cParcels)\n",
    "# print(cTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Parcel Fabric to local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#currently using env.workspace to move parcelfabric needs to be changed to use path\n",
    "pfParcels = 'PROD.MAPPINGADMIN.ParcelFabric_Parcels'\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "a = pfParcels.split('.')[-1]\n",
    "b = f'{a}_{dStr}'\n",
    "# print(f'{iEPath[1]}')\n",
    "if arc.Exists(f'{locGDB / b}'):\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'{b} already exists')\n",
    "else:\n",
    "    print(f'Copying Parcel Fabric to {locGDB}')\n",
    "    #print(a)\n",
    "    # g = f'{f}'\n",
    "    #print(g)\n",
    "    arc.FeatureClassToFeatureClass_conversion(pfParcels, f'{locGDB}', b, where_clause=exp)\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create points out of Parcel Fabric to be used with Permits.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create Points out of polygons in ParcelFabric\n",
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "# print(pfFC)\n",
    "pfPoints = locGDB / f'pfPoints_{dStr}'\n",
    "# print(pfPoints)\n",
    "if arc.Exists(f'{pfPoints}'):\n",
    "    print(f'{pfPoints.name} already exists.')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(pfFC), f'{pfPoints}', \"INSIDE\")\n",
    "    print(f'{pfPoints.name} has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get List of Fields from Parcel Fabric, then create DataFrame for processing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pfFields = [f.name for f in arc.ListFields(pfFC)]\n",
    "# print(pfFields)\n",
    "pf_df = features.GeoAccessor.from_featureclass(pfPoints, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')\n",
    "# pf_df.shape"
   ]
  },
  {
   "source": [
    "Create DataFrame from Parcel Fabric Polygons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "ppf_df = features.GeoAccessor.from_featureclass(pfFC, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create list of columns in DataFrame to check for missing columns. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pfC = pf_df.columns.tolist()\n",
    "# print(pfC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGO Permits DF Creation and Data Curation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create DataFrame from MGO Table </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mTab = [f for f in cTables if f.name.startswith('MGOPermits') == True][0]\n",
    "# mTable = str(tLoc / mgoT)\n",
    "# print(mTable)\n",
    "df_mgo = features.GeoAccessor.from_table(mTab, skip_nulls= False)\n",
    "print(f'{mTab.name} has been created')\n",
    "# df_mgo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create DataFrame for Permits Description to use for merge with MGO Permits. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to local GIS Folder for Processing\n",
    "appCsv = netDir / 'ParcelWithSuffix' / 'Permits_Desc.csv'\n",
    "locCsv = pProcessing / 'Permits_Desc.csv'\n",
    "# print(locCsv)\n",
    "copy2(appCsv, locCsv)\n",
    "print(f'Copied {locCsv.name} to {locCsv.parent}')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "p_desc = pd.read_csv(locCsv, keep_default_na=False)\n",
    "print('Created Permits Description DataFrame')\n",
    "# p_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns and values have been fixed on the spreadsheet. Does not to be automated due to it never changing.\n",
    "#not for us in production\n",
    "# cFix = p_desc.columns.tolist()\n",
    "# pList = [f.strip() for f in cFix]\n",
    "# print(pList)\n",
    "\n",
    "# cpFix = dict(zip(cFix, pList))\n",
    "# print(cpFix)\n",
    "\n",
    "# p_desc = p_desc.rename(columns=cpFix)\n",
    "# # p_desc\n"
   ]
  },
  {
   "source": [
    "Edit the MGO Dates and format them to match AS400 Data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper Case all the column names\n",
    "df_mgo = df_mgo.rename(columns=lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdate(field):\n",
    "    from datetime import date\n",
    "    po = date(2021, 1, 1)\n",
    "    if field >= po:\n",
    "        return field\n",
    "    else:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.CODATE = df_mgo.CODATE.apply(fdate)\n",
    "# df_mgo['CODATE'] = pd.to_datetime(df_mgo.CODATE)\n",
    "# df_mgo.drop(columns='CODate1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mgo.CODATE.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.ISSUEDATE = df_mgo.ISSUEDATE.apply(fdate)\n",
    "# df_mgo['ISSUEDATE'] = pd.to_datetime(df_mgo.CODATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.APPDATE = df_mgo.APPDATE.astype(str)\n",
    "df_mgo.ISSUEDATE = df_mgo.ISSUEDATE.astype(str)\n",
    "df_mgo.CODATE= df_mgo.CODATE.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_mgo['CODATE'] = df_mgo['CODATE'].apply(date)\n",
    "# df_mgo.CODATE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.ISSUEDATE.replace('NaT', '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date(field):\n",
    "    if field != '':\n",
    "        sp = field.split('-')\n",
    "        return '{}/{}/{}'.format(sp[1], sp[-1], sp[0])\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.APPDATE = df_mgo.APPDATE.apply(date)\n",
    "df_mgo.CODATE = df_mgo.CODATE.apply(date)\n",
    "df_mgo.ISSUEDATE = df_mgo.ISSUEDATE.apply(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_mgo.drop(columns='OBJECTID', inplace=True)\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_mgo['PERMITYEAR'] = df_mgo['PERMITNUMBER'].apply(lambda x: x.split('-')[0])\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo = df_mgo.rename(columns={'APPTYPE' : 'MGOTYPE', 'PERMITSTATUS': 'MGOSTATUS'})\n",
    "# df_mgo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo['TYPE'] = df_mgo['PERMITNUMBER'].apply(lambda x: x.split('-')[-1])\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To find the MGO permits that are not in 2021\n",
    "# df_mgo_drop = df_mgo.loc[df_mgo['PERMITYEAR'] != \"2021\", :]\n",
    "# df_mgo_drop\n",
    "#drop the permits not in 2021\n",
    "df_mgo = df_mgo.loc[df_mgo['PERMITYEAR'] == \"2021\", :]\n",
    "# df_mgo\n",
    "# df_mgo_drop.to_csv\n",
    "# df_mgo = df_mgo.loc[df_mgo['PERMITYEAR'] == '2021']\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mGODict = df_mgo.to_dict('records')\n",
    "# print(mGODict)\n",
    "mCSV = pReview / 'mgoPerms.csv'\n",
    "df_mgo.to_csv(mCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export for Review later to see the outer merge which will return all the values and which tables they are located in.<br>\n",
    "Merge to get status and Description from Permit Description DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export for Review later to see the outer merge which will return all the values and which tables they are located in. \n",
    "df_mgo_desc_o = df_mgo.merge(p_desc[['TYPE', 'DESCRIPTION']], how='outer', on='TYPE', indicator= True) #merge to create description column\n",
    "tEr = pReview / 'Type_Error.csv'\n",
    "df_mgo_desc_o.loc[df_mgo_desc_o._merge == 'left_only', :].to_csv(tEr, index=False) # export csv if any errors\n",
    "df_mgo_desc_o = df_mgo_desc_o.drop(columns='_merge')\n",
    "\n",
    "#merge to get status from Permit Description DataFrame\n",
    "df_mgo_mis = df_mgo_desc_o.merge(p_desc[['MGO_DESC', 'MGO_INIT']], how='outer',\n",
    "left_on='MGOSTATUS', right_on='MGO_DESC', indicator=True) #merge to create status column\n",
    "mgoM = pReview / 'Status_Error.csv'\n",
    "df_mgo_mis.loc[df_mgo_mis._merge == 'left_only'].to_csv(mgoM, index=False)\n",
    "mgo_df = df_mgo_mis.loc[df_mgo_mis._merge == 'both']\n",
    "# df_mgo_mis[df_mgo_mis._merge == 'both']\n",
    "# merMTable = join(locFolder, f'MGOPermitsMerge_{dStr}.csv')\n",
    "#not for production, used to find null values\n",
    "# # df_mgo_mis.loc[df_mgo_mis['_merge'] != 'both', :]\n",
    "# # df_mgo_mis.loc[df_mgo_mis['_merge'] == 'left_only', :]\n",
    "# df_mgo_mis.to_csv(merMTable, index=False)\n",
    "# del df_mgo_desc_o\n",
    "print(\"Exported MGO Permits Merge Table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mgo_mis[df_mgo_mis.PARCELID == '7-00-10302-01-4300-00001']\n",
    "# df_mgo_mis.loc[(df_mgo_mis.PARCELID.duplicated() == True) & (df_mgo_mis._merge == 'both'), :].groupby(by='PARCELID')\n",
    "# print(dList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All_Permits DF Creation and Data Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create DataFrame from All_Permits table on Local GDB after exporting from ESRIDB. If arc.env.workspace is changed need to change aPerms to full path.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTab = [f for f in cTables if f.name.startswith('All_Permits') == True][0]\n",
    "# mTable = str(tLoc / mgoT)\n",
    "# print(mTable)\n",
    "ap_df = features.GeoAccessor.from_table(aTab, skip_nulls= False)\n",
    "print(f'{aTab.name} has been created')\n",
    "# ap_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove whitespace and make it upper\n",
    "ap_df.rename(columns=lambda x: x.strip().upper(), inplace=True)\n",
    "ap_df.drop(columns='OBJECTID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix a blank entry from PRIDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ap_df['PARCELID'] = ap_df['PARCELID'].apply(lambda x: '7-00-12900-02-5200-00001' if x.strip() == '-00-12900-02-5200-00001' else x)"
   ]
  },
  {
   "source": [
    "Create Date columns for sorting by date later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def newyear(field):\n",
    "    if int(field) > 80 and int(field) < 100:\n",
    "        return \"19\" + str(field)\n",
    "    elif int(field) >= 0 and int(field) <=9:\n",
    "        return \"20\" + str(field)\n",
    "    elif int(field) > 9 and int(field) < 80:\n",
    "        return \"20\" + str(field)\n",
    "ap_df['YEAR'] = ap_df['YEAR'].apply(newyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_ap['YEAR_FORM'] = df_ap['YEAR'].apply(lambda x: f'{x[0:3]}{x[-1]}') For Correction of mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ap_df['PERMITNUMBER'] = ap_df[['YEAR', 'PERMITNUMBER', 'TYPE']].apply(lambda x: f'{x[0]}-{x[1]}-{x[2]}', axis=1)\n",
    "ap_df = ap_df[ap_df.YEAR != '2021']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df.drop(columns='DESCRIPTION', inplace=True)\n",
    "ap_df_desc = ap_df.merge(p_desc[['TYPE', 'DESCRIPTION']], on='TYPE', how='outer', indicator= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df_b = ap_df_desc[ap_df_desc._merge == 'both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df_b.drop(columns=['PERMITDESC', '_merge'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap_m.loc[ap_m.SHAPE.isnull(), :]\n",
    "ap_m.spatial.set_geometry('SHAPE', sr=26957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lal = gisPath / 'HPermits.csv'\n",
    "ap_m.to_csv(lal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "papa = pd.read_csv(lal)\n",
    "p2p = gpd.GeoDataFrame(papa, geometry='SHAPE', crs=26957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmdf = features.GeoAccessor.from_df(ap_m, sr=26957, geometry_column='SHAPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Feature Class for Historical Data, contains some geospatial information.\n",
    "hPFC = locGDB / f'Historical_Permits_{dStr}'\n",
    "ap_m = pf_df.merge(ap_df_b, how='outer', left_on='Name', right_on='PARCELID', indicator=True)\n",
    "ap_m = ap_m[ap_m._merge != 'left_only']\n",
    "ap_m.drop(columns='_merge', inplace=True)\n",
    "ap_m.fillna('', inplace=True)\n",
    "\n",
    "a = ap_m.spatial.to_featureset()\n",
    "a.save(f'{outGDB}', 'jo')\n",
    "\n",
    "# ap_m.spatial.to_featureclass(f'{hPFC}', sanitize_columns=False)\n",
    "\n",
    "print(f'Exported Historical Permits to {hPFC}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_df_b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = features.GeoAccessor.from_df(ap_m, geometry_column='SHAPE')\n",
    "ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_m.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ap_m.spatial.to_featureset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.save(f'{hPFC.parent}', f'{hPFC.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_ap = ap_df_b.sort_values(by='YEAR')\n",
    "dup_ap = ap_df_b.drop_duplicates(subset=['YEAR', 'PARCELID', 'TYPE'], keep='last', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop RJ and HD status from the permits\n",
    "dup_ap = dup_ap.loc[(dup_ap.STATUS != 'RJ') | (dup_ap.STATUS != 'HD')]\n",
    "# dup_ap.STATUS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_ap = dup_ap.merge(p_desc[['AS_DESC', 'AS_INIT']], how='left', left_on='STATUS', right_on='AS_INIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aPerm = pReview / 'AS_Perms.csv'\n",
    "dup_ap.to_csv(aPerm, index=False)"
   ]
  },
  {
   "source": [
    "## Merge MGO Permits and AS400 Permits into one table and get geospatial information from Parcel Fabric Points."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop and rename columns from AS400 and MGO DataFrames\n",
    "mgo_df.drop(columns='_merge', inplace=True)\n",
    "dup_ap.drop(columns='AS_INIT', inplace=True)\n",
    "dup_ap.rename(columns={'AS_DESC' : 'STATDESC'}, inplace=True)\n",
    "\n",
    "#Remove Parcel #s that are in AS400 Data (dup_ap)\n",
    "pmf = mgo_df[~mgo_df.PARCELID.isin(dup_ap.PARCELID)]\n",
    "pmf.rename(columns={'AS_DESC' : 'STATDESC'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the pmf with dup_ap to get a list of all permits from MGO and AS400\n",
    "perm_df = pmf.merge(dup_ap, on='PARCELID', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the columns into one.\n",
    "perm_df['TYPE'] = perm_df.TYPE_x.str.cat(perm_df.TYPE_y, na_rep='')\n",
    "perm_df['PERMITNUMBER'] = perm_df.PERMITNUMBER_x.str.cat(perm_df.PERMITNUMBER_y, na_rep='')\n",
    "perm_df['DESCRIPTION'] = perm_df.DESCRIPTION_x.str.cat(perm_df.DESCRIPTION_y, na_rep='')\n",
    "perm_df['APPDATE'] = perm_df.APPDATE_x.str.cat(perm_df.APPDATE_y, na_rep='')\n",
    "perm_df['CODATE'] = perm_df.CODATE_x.str.cat(perm_df.CODATE_y, na_rep='')\n",
    "perm_df['ISSUED'] = perm_df.ISSUEDATE.str.cat(perm_df.DATE, na_rep='')\n",
    "perm_df['STATUSDESC'] = perm_df.STATDESC.str.cat(perm_df.MGO_DESC, na_rep='')\n",
    "perm_df['PERMSTATUS'] = perm_df.STATUS.str.cat(perm_df.MGO_INIT, na_rep='')\n",
    "perm_df['PERMYEAR'] = perm_df.YEAR.str.cat(perm_df.PERMITYEAR, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to keep for the Feature Class on ESRIDB and the REST Server\n",
    "pmCols = ['PARCELID', 'TYPE', 'PERMSTATUS', 'ISSUED', 'PERMYEAR', 'PERMITNUMBER', 'DESCRIPTION', 'SHAPE', 'CODATE', 'APPDATE']\n",
    "pmFields = [f.name for f in arc.ListFields('PROD.GISADMIN.Permits')][1:]\n",
    "pmFields.remove('PermitNumb')\n",
    "kpmCols = dict(zip(pmCols, pmFields))\n",
    "kpmCols['STATUSDESC'] = 'StatusDesc'\n",
    "pmCols.append('STATUSDESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = perm_df.drop(columns=[f for f in perm_df.columns if f not in pmCols])\n",
    "#Rename Columns\n",
    "pm_df.rename(columns=kpmCols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df.AppDate = pd.to_datetime(pm_df.AppDate, format='%m/%d/%Y', errors='coerce')\n",
    "pm_df.IssueDate = pd.to_datetime(pm_df.IssueDate, format='%m/%d/%Y', errors='coerce')\n",
    "pm_df.PermitYear = pd.to_datetime(pm_df.PermitYear, format='%Y', errors='coerce')\n",
    "pm_df.CODate = pd.to_datetime(pm_df.CODate, format='%m/%d/%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare Parcel Fabric Points and Merge with Permit Data\n",
    "pf_df.Name = pf_df.Name.apply(lambda x: x.strip())\n",
    "pfm_df = pm_df.merge(pf_df, on='Name', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_pm is all the permits with suffix and t_pm is parcels that aren't in Parcel Fabric\n",
    "sOut = pReview / 'Permits_Suffix.csv'\n",
    "tOut = pReview / 'Permits_Parcels_NM.csv' #NM = No Match\n",
    "s_pm = pfm_df[(pfm_df._merge == 'left_only') & (~pfm_df.Name.str.contains('-00001', na=False))]\n",
    "t_pm = pfm_df[(pfm_df._merge == 'left_only') & (pfm_df.Name.str.contains('-00001', na=False))]\n",
    "s_pm.to_csv(sOut, index=False)\n",
    "t_pm.to_csv(tOut, index=False)\n",
    "\n",
    "print(f'Exported {sOut.name} and {tOut.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Permits to GDB and append to Permits on the DB. Not all Permits contain GeoSpatial Information. \n",
    "perm_db = locGDB / 'Permits_DB'\n",
    "\n",
    "db_pm = pfm_df[pfm_df._merge != 'right_only']\n",
    "db_pm.drop(columns='_merge', inplace=True)\n",
    "# db_pm.spatial.to_featureclass(perm_db, sanitize_columns=False)"
   ]
  },
  {
   "source": [
    "### Create DEMO Permits Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Create AS400 Demo Permits DataFrame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Export Demo Permits for Review\n",
    "dCsv = pReview / 'DemoPermits.csv'\n",
    "df_demo = pm_df.query('StructureType == \"DEMO\" | StructureType == \"DEMS\" | StructureType == \"MHDM\"')\n",
    "# df_demo.reset_index(drop=True, inplace=True)\n",
    "df_demo.to_csv(dCsv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Parcel Fabric Polygons and df_demo Data\n",
    "pf = features.GeoAccessor.from_featureclass(f'{pfFC}', fields=['Name'])\n",
    "pf.Name = pf.Name.apply(lambda x: x.strip())\n",
    "demo_fc = pf.merge(df_demo, left_on='Name', right_on='Name', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo_fc.loc[(demo_fc._merge == 'left_only') & (demo_fc.PARCELID.str.contains('-00001', na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Parcel Names that don't match\n",
    "dLo = pReview / 'Missing_Demo_P.csv'\n",
    "demo_fc[demo_fc._merge == 'left_only'].to_csv(dLo, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fc = demo_fc[demo_fc._merge == 'both']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_fc.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to keep for the Feature Class on ESRIDB for employees only\n",
    "ddCols = ['PARCELID', 'TYPE', 'PERMITNUMBER', 'DESCRIPTION', 'APPDATE', 'CODATE', 'ISSUED', 'STATUSDESC', 'PERMSTATUS', 'PERMYEAR', 'SHAPE']\n",
    "db_demo = demo_fc.drop(columns=[f for f in demo_fc.columns if f not in ddCols])\n",
    "#Rename Columns\n",
    "db_demo.rename(columns={'PERMYEAR' : 'YEAR', 'ISSUED': 'ISSUEDATE'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Match the Columns to the existing Demo Permits Feature Class for use with the REST Server\n",
    "dkCols = ['Name', 'TYPE', 'PERMSTATUS', 'DESCRIPTION', 'SHAPE']\n",
    "rest_demo = demo_fc.drop(columns=[x for x in demo_fc.columns if x not in dkCols])\n",
    "\n",
    "cols = {'TYPE': 'DemoType',\n",
    "        'STATUS': 'Status',\n",
    "        'DESCRIPTION': 'Descriptio'}\n",
    "rest_demo.rename(columns=cols, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demoRest = locGDB / f'Demo_Perms_REST_{dStr}'\n",
    "demoEmp = locGDB / f'Demo_Perms_DB_{dStr}'\n",
    "# de_fc = features.GeoAccessor.from_df(demo_fc, geometry_column='SHAPE')\n",
    "# db_demo.spatial.to_featureclass(f'{demoEmp}', sanitize_columns=False)\n",
    "rest_demo.spatial.to_featureclass(f'{demoRest}', sanitize_columns=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dupYearsF = []\n",
    "# for field in range(len(dupYears)):\n",
    "#     if int(dupYears[field]) > 80 and int(dupYears[field]) < 100:\n",
    "#         dupYearsF.append(\"19\" + str(field))\n",
    "#     elif int(field) >= 0 and int(field) <=9:\n",
    "#         dupYearsF.append(\"20\" + str(field))\n",
    "#     elif int(field) > 9 and int(field) < 80:\n",
    "#          dupYearsF.append(\"20\" + str(field))\n",
    "# print(dupYearsF)"
   ]
  },
  {
   "source": [
    "### Create Permits Feature Class for REST Server"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<b> Export OWNERSHIPINFORMATION Table to local GDB. Change the path if setting environment variable. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "owTable = wsDB + '\\\\PROD.gisadmin.OWNERSHIPINFORMATION'\n",
    "owOutTable = f'OwnerTable_{dStr}'\n",
    "\n",
    "arc.TableToTable_conversion(owTable, locGDB, owOutTable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# arc.ConvertTimeField_management use this when exporting the feature classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0be568a45c5be0d4caf76dff1c2f5b3b93950ef2ed99d2899e809934a8a7c9d46",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}