{
 "cells": [
  {
   "source": [
    "# Create Feature Class with the Sheriff Sales values, Add to Employee Map, Add to Sheriff Map, and send an email to Kathy Skinner telling her of the mismatched values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " <b> TO DO: </b>\n",
    " <ul> \n",
    "        <li><b><del>Edit Address DataFrame for merge in case the ParcelID merge doesn't have a match.</b></li>\n",
    "        <li><b>Use spatial join when the parcels do not match. Test spatial join with python api. If not use geopandas spatial join.</b></li>\n",
    "        <li><b>Do not use arc.env.workspace Environment Variables.</b></li>\n",
    "        <li><b>Import dotenv to get rid of login credentials.</b></li>\n",
    "        \n",
    "        \n",
    "        \n",
    "         </ul> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules to import\n",
    "import arcpy as arc\n",
    "import pandas as pd\n",
    "from arcgis import GIS, features\n",
    "import csv\n",
    "# import pendulum\n",
    "from pathlib import Path\n",
    "from shutil import copy2 as cp\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### FOR USE WITH NOTEBOOK ONLY"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 40"
   ]
  },
  {
   "source": [
    "## Curate the CSV file to be used with ArcGIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Variables\n",
    "fMon = pendulum.datetime(2021, 4, 1)\n",
    "fMon_Str = fMon.strftime('%m/%d/%Y')\n",
    "today = pendulum.today()\n",
    "tStr = today.strftime('%m/%d/%Y')\n",
    "# print(tStr)\n",
    "now = pendulum.now()\n",
    "mDate = now.strftime('%m%Y')\n",
    "dDate =  now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "revPath = uPath / 'GIS' / 'Review'\n",
    "gisPath = uPath / 'GIS' / 'Processing'\n",
    "netDir = Path(r'\\\\esripub2\\SalesWebExport')\n",
    "netCsv = netDir / 'SalesWebExport.csv'\n",
    "sPath = revPath / 'Sheriff' / f'{dDate}'\n",
    "sPath.mkdir(parents=True, exist_ok=True)\n",
    "dropFields = ['StreetNumber', 'StreetPrefix', 'Address1', 'StreetType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the csv file from network drive to local drive for processing and create DataFrame \n",
    "try:\n",
    "    cp(netCsv, gisPath)\n",
    "    print(f'Copied {netCsv} to {gisPath}')\n",
    "except:\n",
    "    print(f'Could not copy {netCsv}')\n",
    "# in the script check to make sure that the user has access to the server and can copy the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can use Pandas, CSV module, or generators/iterators\n",
    "lines = (line for line in netCsv.open())\n",
    "list_line = (s.rstrip().split(',') for s in lines)\n",
    "cols = next(list_line)  \n",
    "# print(list(data))\n",
    "sher_dict = (dict(zip(cols,data)) for data in list_line)\n",
    "d = [k for k,v in sher_dict]\n",
    "print(d) \n",
    "# jAmount = (p['\"ApproxJudgment\"'] for p in sher_dict)\n",
    "# l = [k for k,v in sher_dict]\n",
    "# print(f'Blank {l}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(netCsv, na_filter= False)\n",
    "print(f\"Created DF from {netCsv.name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage Data by changing the Prefix to match the Address Format in the Address Feature Class\n",
    "columns = sales_df.columns.to_list()\n",
    "# print(columns)\n",
    "pre = sales_df.StreetPrefix.unique()\n",
    "# print(pre)\n",
    "for p in pre:\n",
    "    if p.upper() == 'NORTH':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'N').strip())\n",
    "    elif p.upper() == 'EAST':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'E').strip())\n",
    "    elif p.upper() == 'WEST':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'W').strip())\n",
    "    elif p.upper() == 'SOUTH':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'S').strip())\n",
    "    else:\n",
    "        sales_df.StreetPrefix\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage Data by changing the Prefix to match the Address Format in the Address Feature Class\n",
    "sType = sales_df.StreetType.unique()\n",
    "# print(sType)\n",
    "for d in sType:\n",
    "    if d.upper() == 'ROAD':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'RD').strip())\n",
    "    elif d.upper() == 'DRIVE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "    elif d.upper() == 'BOULEVARD':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "    elif d.upper() == 'LANE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "    elif d.upper() == 'COURT':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "    elif d.upper() == 'CIRCLE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "    elif d.upper() == 'STREET':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "    elif d.upper() == 'HIGHWAY':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "    elif d.upper() == 'AVENUE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "    elif d.upper() == 'PLACE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "    else:\n",
    "        sales_df.StreetType.str.strip()\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Address from other fields and remove white space\n",
    "# for d in pre:\n",
    "#     if d == ' ':\n",
    "#         sales_df['Address'] = sales_df.StreetNumber + ' ' + sales_df.Address1 + ' ' + sales_df.StreetType      \n",
    "#     else:\n",
    "#         sales_df['Address'] = sales_df.StreetNumber + ' ' + sales_df.StreetPrefix + ' ' + sales_df.Address1 + \\\n",
    "#         ' ' + sales_df.StreetType\n",
    "# sourceCol = sales_df.columns.get_loc('StreetNumber')\n",
    "sales_df['Add'] = sales_df[['StreetNumber', 'StreetPrefix', 'Address1', 'StreetType']].values.tolist()\n",
    "sales_df['Addr'] = sales_df['Add'].apply(' '.join)\n",
    "sales_df['Address'] = sales_df['Addr'].apply(lambda x: ' '.join(x.split()))\n",
    "# sales_df.iloc[sales_df[''].apply\n",
    "sales_df.drop(columns=['Add', 'Addr'], inplace=True)\n",
    "# sales_df.assign(Address = )\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If Address2 field is notnull or na then move value into Address field. Do this after address matching when using df_add to merge.\n",
    "if len(sales_df.loc[sales_df.Address2 != '']) > 0:\n",
    "    sales_df.Address = np.where(sales_df.Address2 != '', sales_df.Address2, sales_df.Address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dates and remove dates that are before current date\n",
    "sales_df['PropStatus'] = sales_df.PropertyStatusDate\n",
    "sales_df.PropertyStatusDate = pd.to_datetime(sales_df.PropertyStatusDate, format='%m/%d/%Y', errors='ignore', exact=True)\n",
    "\n",
    "#Change fMon_Str to tStr when removing by current date\n",
    "sales_df = sales_df.loc[sales_df.PropertyStatusDate >= fMon_Str, :]\n",
    "sales_df.drop(columns='PropertyStatusDate', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out certain action types and property statuses\n",
    "sales_df = sales_df.loc[sales_df.ActionTypeDescription != 'Personal Property']\n",
    "sales_df = sales_df.loc[sales_df.PropertyStatusDescription != 'Stayed']\n",
    "\n",
    "#need to create a note section if the parcel number contains a &. Call count to see if there are any values that meet this requirement. Need to remove the & from the column\n",
    "sValue = sales_df.ParcelNumber.str.count('\\&').tolist()\n",
    "for s in sValue:\n",
    "    if s == 1:\n",
    "        sales_df['Notes'] = sales_df.loc[sales_df.ParcelNumber.str.contains('\\&')].values + ': This parcel is also included in the listed sale'\n",
    "    else:\n",
    "        print('There are no additional parcels.')\n",
    "# sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holds all the values with a Parcel Number\n",
    "sales_df.ParcelNumber = sales_df.ParcelNumber.replace('', np.nan)\n",
    "sales_df\n",
    "#Handle missing Parcel Number values\n",
    "nan_df = sales_df.loc[sales_df.ParcelNumber.isna(), :]\n",
    "print(len(nan_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable to hold the number of values in each df to not forget any\n",
    "sValues = sales_df.ParcelNumber.count().tolist()\n",
    "print(sValues)\n",
    "nValues = nan_df.SheriffNumber.count().tolist()\n",
    "print(nValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CSV for nan values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_df_test['Address'] = sales_df_test['Address'].apply(lambda x: x.strip())\n",
    "# lol = sales_df.Addr.unique().tolist()\n",
    "# print(lol)\n",
    "# for x in lol:\n",
    "#     print(x + ' ' + str(len(x)))\n",
    "# for x in lol:\n",
    "#     a = ' '.join(x.split())\n",
    "#     print(x + ' ' + str(len(a)))"
   ]
  },
  {
   "source": [
    "Check to see if there are any parcel #'s with an & in the field. If so writes the value to a new column called \"Notes\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = '9-00-1111 & aksdasd0asdasd'\n",
    "# print(x.split('&')[-1])\n",
    "# ePar = sales_df.ParcelNumber.unique()\n",
    "# for p in ePar:\n",
    "#     if len(p.split('&')[-1]) > 1:\n",
    "#         print(p)\n",
    "#     else:\n",
    "#         print('Nope')\n",
    "idx = sales_df.ParcelNumber.apply(lambda x: x.split('&')[-1])\n",
    "# print(idx)\n",
    "sales_df['Notes'] = ''\n",
    "sales_df.loc[idx, 'Notes'] = str(sales_df.ParcelNumber.str.contains('&', na=False)).split('&')[-1] + ': This parcel is also included in the listed sale'\n",
    "# sales_df\n",
    "# sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addressing and Parcels using spatial DataFrame from arcgis to merge\n",
    "#Need to change these from the hard values when running the script\n",
    "locGDB = gisPath / 'Data_042021.gdb' / f'Daily_{dDate}'\n",
    "gdbPar = locGDB / f'ParcelFabric_Parcels_{dDate}'\n",
    "gdbAdd = locGDB / f'Addressing_{dDate}'\n",
    "\n",
    "df_add = features.GeoAccessor.from_featureclass(gdbAdd)\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "df_par = features.GeoAccessor.from_featureclass(gdbPar, where_clause=exp, fields=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame from merge with parcels and sales\n",
    "merge_par = sales_df.merge(df_par, how='outer', right_on='Name', left_on='ParcelNumber', indicator=True)\n",
    "# merge_par.columns\n",
    "iList = merge_par._merge.unique().tolist()\n",
    "print(iList)\n",
    "#Create csvs from merge for review later\n",
    "for i in iList:\n",
    "    if i == 'right_only':\n",
    "        a = sPath / 'Par_Sales_Right.csv'\n",
    "        merge_par.loc[merge_par._merge == 'right_only', :].to_csv(a, index=False)\n",
    "        print(f'Created {a.stem}')\n",
    "    elif i == 'left_only':\n",
    "        a = sPath / 'Par_Sales_Left.csv'\n",
    "        merge_par.loc[merge_par._merge == 'left_only', :].to_csv(a, index=False)\n",
    "        print(f'Created {a.stem}')\n",
    "    elif i == 'both':\n",
    "        a = sPath / 'Par_Sales_Both.csv'\n",
    "        merge_par.loc[merge_par._merge == 'both', :].to_csv(a, index=False)\n",
    "        print(f'Created {a.stem}')\n",
    "# rCSV = sPath / 'Par_Sales_Right.csv'\n",
    "# lCSV = sPath / 'Par_Sales_Left.csv'\n",
    "# bCsv = sPath / 'Par_Sales_Both.csv'\n",
    "# merge_par.loc[merge_par._merge == 'right_only', :].to_csv(index=False)\n",
    "\n",
    "#Number of values from merge \n",
    "pValues = len(merge_par.loc[merge_par._merge == 'both', :])\n",
    "print(pValues)\n",
    "\n",
    "if pValues == sValues:\n",
    "    merge_par_b = merge_par.loc[merge_par._merge == 'both', :]\n",
    "    merge_par_b.drop(columns=['_merge', 'StreetNumber', 'StreetPrefix', 'StreetType', 'StreetSuffix', 'Address1', 'Address2', 'Address3', 'Name'], inplace=True)\n",
    "    print('All Features are matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to get the fields to match. Most likely will create new feature classes to use. \n",
    "netDir = Path('G:\\\\MasterGISFiles\\\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'\n",
    "iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "arc.env.workspace = f'{iE.parent}'\n",
    "# print(locGDB)\n",
    "fcList = [f for f in arc.ListFeatureClasses(feature_dataset='PROD.GISADMIN.SheriffSales')]\n",
    "print(fcList)\n",
    "fList = [f.name for f in arc.ListFields(f'{iE.parent / \"PROD.GISADMIN.SheriffSales\" / \"PROD.GISADMIN.BothSales\"}')]\n",
    "# for x in fcList:\n",
    "#     a = arc.ListFields(f'{iE.parent / 'PROD.GISADMIN.SheriffSales' / x}')\n",
    "#     for l in a:\n",
    "#         fList.\n",
    "#         fList.append(arc.ListFields(x))\n",
    "print(fList)\n",
    "\n",
    "sCols = sales_df.columns.tolist()\n",
    "cols = dict(zip(sCols, fList))\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_par_b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_par.drop(columns=['StreetNumber', 'StreetPrefix', 'StreetType', 'StreetSuffix', 'Address1', 'Address2', 'Address3', 'Name'], inplace=True)\n",
    "# merge_par = merge_par[['SheriffNumber'] + [col for col in merge_par.columns if col !='SheriffNumber']]\n",
    "# print(list(merge_par_b))\n",
    "# cols = list(merge_par_b)"
   ]
  },
  {
   "source": [
    "If values from merge is zero then use df_add and create spatial join <br>\n",
    "If values from sales_df = values from df_par then create Sheriff feature class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shFC = locGDB / f'Sheriff_Final_{dDate}'\n",
    "if pValues == sValues:\n",
    "    merge_par_b.spatial.to_featureclass(shFC, sanitize_columns=False)\n",
    "    print(f\"Created {shFC.stem} at {locGDB}\")"
   ]
  },
  {
   "source": [
    "### Edit the Address DataFrame (df_add) to replace the null values in FullAddr column. Only use if the Parcel Fabric Layer and the sales speadsheet don't fully match."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add['STS'].replace('None', \" \")\n",
    "# df_add.loc[df_add.STS == None, :]\n",
    "# df_add.isnull().sum()\n",
    "df_add['STS'].fillna(np.nan, inplace= True)\n",
    "df_add['STS'].replace(' ', '', inplace=True)\n",
    "df_add['STS'].replace(np.nan, '', inplace=True)\n",
    "# df_add['STS'].unique()\n",
    "# df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aType = df_add.STS.unique()\n",
    "# print(sType)\n",
    "for d in aType:\n",
    "    if d.upper() == 'ROAD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'RD').strip())\n",
    "    elif d.upper() == 'DRIVE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "    elif d.upper() == 'BOULEVARD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "    elif d.upper() == 'LANE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "    elif d.upper() == 'COURT':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "    elif d.upper() == 'CIRCLE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "    elif d.upper() == 'STREET':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "    elif d.upper() == 'HIGHWAY':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "    elif d.upper() == 'AVENUE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "    elif d.upper() == 'PLACE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "    else:\n",
    "        df_add.STS.str.strip()\n",
    "print(df_add.STS.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatially Join Addressing and Parcels. Need to project Addressing to the same SR\n",
    "# df_add = df_add.spatial.project()\n",
    "# df_join = df_par.spatial.join(df_add, left_tag='p_', right_tag='a_')\n",
    "# df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.fillna(np.nan, inplace= True)\n",
    "# df_add.replace(' ', '', inplace=True)\n",
    "df_add.replace(np.nan, '', inplace=True)\n",
    "# print(df_add.columns)\n",
    "print(df_add.STS.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.SAN = df_add.SAN.astype('str',errors='ignore')\n",
    "df_add.SAN = df_add.SAN.apply(lambda x: x.split('.')[0])\n",
    "df_add['FAddress'] = df_add[['SAN', 'PRD', 'STN', 'STS', 'POD']].values.tolist()\n",
    "# df_add.FAddress.values\n",
    "df_add['FooAddress'] = df_add['FAddress'].apply(' '.join)\n",
    "df_add['FoAddress'] = df_add['FooAddress'].apply(lambda x: ' '.join(x.split()))\n",
    "# df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.FAddress = df_add.FoAddress\n",
    "kColumns = ['FAddress', 'SHAPE']\n",
    "# print([x for x in kColumns if x not in kColumns])\n",
    "df_add.drop(columns=[x for x in df_add if x not in kColumns], inplace=True)\n",
    "df_add"
   ]
  },
  {
   "source": [
    "Create DataFrame from merge with address and sales, create csvs from merge for review later, and get number of values after the merge."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame from merge with address and sales\n",
    "sales_df['lAdd'] = sales_df.Address.str.upper()\n",
    "# del merge_add\n",
    "merge_add = df_add.merge(sales_df, how='outer', left_on='FAddress', right_on='lAdd', indicator=True, )\n",
    "# merge_add\n",
    "\n",
    "#create csvs from merge for review later\n",
    "\n",
    "merge_add[merge_add._merge == 'both']\n",
    "\n",
    "#Number of Values from merge\n",
    "maValues = merge_add.SHAPE.count().tolist()"
   ]
  },
  {
   "source": [
    "Use gdbJoin after editing and joining the address and sales tables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "gdbJoin = locGDB / f'Par_Add_{dDate}'\n",
    "arc.SpatialJoin_analysis(f'{gdbPar}', f'{gdbAdd}', f'{gdbJoin}')\n",
    "df_join = features.GeoAccessor.from_featureclass(gdbJoin)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Columns from df_par not needed for merge\n",
    "# pkColumns = ['Name', 'SHAPE']\n",
    "# df_par.drop(columns=[x for x in df_par if x not in pkColumns], inplace=True)\n",
    "# df_par.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0be568a45c5be0d4caf76dff1c2f5b3b93950ef2ed99d2899e809934a8a7c9d46",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}