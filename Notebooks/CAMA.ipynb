{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Parcels with Suffix Feature Class from CAMA Data and AS400 Data."
   ]
  },
  {
   "source": [
    " <b> TO DO: </b>\n",
    " <ul> \n",
    "        <li><del>Automate the process of editing the df_Pride_L CSV.</del> No longer applicable.</li>\n",
    "        <li>Do not use arc.env.workspace Environment Variables.</li>\n",
    "        <li><del>Edit Addresses in DataFrame for better matching.</li>\n",
    "        <li><del>Import dotenv to get rid of login credentials.</li>\n",
    "        <li><del>Fix Edits CSV PARENTS column with correct values.</li>\n",
    "        <li>Geocode remaining addresses if those don't match merge PARENTS column with Parcel Fabric to get spatial data.</li>\n",
    "        <li>Possible merge with the rest of the MGO Data.</li>\n",
    "        \n",
    "        \n",
    "         </ul> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and declare globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use arcNew environment with this notebook.\n",
    "#Current Location: C:\\ProgramData\\Anaconda3\\envs\\arcNew\\python.exe\n",
    "import arcpy as arc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copy2\n",
    "from pathlib import Path\n",
    "from arcgis import features\n",
    "from datetime import datetime as dt\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folders in the Users Directory since it should have r/w permissions for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "now = dt.now()\n",
    "mStr = now.strftime('%m%Y')\n",
    "dStr = now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'"
   ]
  },
  {
   "source": [
    "### Only use with notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pd.options.display.max_columns = 40"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create File GeoDatabase and Feature Datasets"
   ]
  },
  {
   "source": [
    "### Create Folders for Parcels with Suffix Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create Folders for ParcelWithSuffix Data\n",
    "psFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "psProcessing = psFolder / 'ParcelWithSuffix' / f'{dStr}'\n",
    "if psProcessing.exists() == True:\n",
    "    print(f'{psProcessing} already exist.')\n",
    "else:\n",
    "    psProcessing.mkdir(parents=True)\n",
    "    print(f'Created {psProcessing}.')\n",
    "\n",
    "psFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "psReview = psFR / 'ParcelWithSuffix' / f'{dStr}'\n",
    "if psReview.exists() == True:\n",
    "    print(f'{psReview} already exist')\n",
    "else:\n",
    "    psReview.mkdir(parents=True)\n",
    "    print(f'Created {psReview}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "sr = arc.Describe(f'{iE}').spatialReference\n",
    "outGDB = gisPath / psFolder / f'Data_{mStr}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dStr}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{psFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(5)\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tax Parcels and Addressing FC into LOCAL GDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Does not work with Parcel Fabric due to bug with 10.5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbAdd = 'PROD.ADDRESSINGADMIN.Addressing'\n",
    "dbSufP = 'PROD.GISADMIN.Suffix_Parcels'\n",
    "dbMPTab = 'PROD.gisadmin.MGOPermits'\n",
    "dbAPTab = 'PROD.gisadmin.All_Permits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create paths to the data instead of using env.workspace makes it so that I can use arc Walk\n",
    "arc.env.workspace = f'{iE.parent}'\n",
    "\n",
    "fcList = [dbAdd, dbSufP]\n",
    "tList = [dbMPTab, dbAPTab]\n",
    "cParcels = []\n",
    "cTables = []\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in fcList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}'\n",
    "            if arc.Exists(f'{locGDB / b}'):\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.FeatureClassToFeatureClass_conversion(f, f'{locGDB}', f'{b}')\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in tList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}' #Tables need to have a date string since they won't be under the dataset\n",
    "            # c = locGDB.parent\n",
    "            if arc.Exists(f'{outGDB / b}'):\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.TableToTable_conversion(f, f'{outGDB}', f'{b}')\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "# print(cParcels)\n",
    "# print(cTables)\n",
    "\n",
    "# Copying Suffix_Parcels\n",
    "# cList = ['PROD.GISADMIN.Suffix_Parcels']\n",
    "# iEPath = list(map(lambda x: iE.parent / x, cList))\n",
    "# print(f'{iEPath[0]}')\n",
    "\n",
    "# for f in iEPath:\n",
    "#     print(f'Copying FCs to {locGDB}')\n",
    "#     a = str(f).split('.')[-1]\n",
    "#     b = f'{a}_{dStr}'\n",
    "#     print(a)\n",
    "#     g = f'{f}'\n",
    "#     print(g)\n",
    "#     arc.FeatureClassToFeatureClass_conversion(g, f'{locGDB}',b)\n",
    "#     cParcels.append(locGDB / b)\n",
    "#     print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works with Parcel Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#currently using env.workspace to move parcelfabric needs to be changed to use path\n",
    "pfParcels = 'PROD.MAPPINGADMIN.ParcelFabric_Parcels'\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "a = pfParcels.split('.')[-1]\n",
    "b = f'{a}_{dStr}'\n",
    "# print(f'{iEPath[1]}')\n",
    "if arc.Exists(f'{locGDB / b}'):\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'{b} already exists')\n",
    "else:\n",
    "    print(f'Copying Parcel Fabric to {locGDB}')\n",
    "    #print(a)\n",
    "    # g = f'{f}'\n",
    "    #print(g)\n",
    "    arc.FeatureClassToFeatureClass_conversion(pfParcels, f'{locGDB}', b, where_clause=exp)\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "source": [
    "### Create SQL connection to copy the Pride Main Table. After that is done create DataFrame from Pride Main Table."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List to hold all the DataFrames for concat/merge at the end\n",
    "allDF = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Pride Main Table from KCC-SQL1\\Assess50Kent\n",
    "import pyodbc as db\n",
    "server = 'KCC-SQL1'\n",
    "database = 'Assess50Kent'\n",
    "try:\n",
    "    cnxn = db.connect('DRIVER={ODBC Driver 17 for SQL Server};''SERVER='+server+';DATABASE='+database+';TRUSTED_CONNECTION=yes;')\n",
    "    print(f'Connected to {database}')\n",
    "except:\n",
    "    print(f'Could not connect to {database}')\n",
    "\n",
    "cur = cnxn.cursor()\n",
    "kColumns = ['ParcelID', 'BillingAddress', 'BillingAddress2', 'LocationZip', 'FormattedLocation', 'Owner1', 'Lot']\n",
    "# cColumns = [c.column_name for c in cur.columns(table='PrideMain')]\n",
    "q = f\"Select {kColumns[0]}, {kColumns[1]}, {kColumns[2]}, {kColumns[3]} as ZipCode, {kColumns[4]}, {kColumns[5]} as Owner, {kColumns[-1]} FROM PrideMain WHERE {kColumns[0]} NOT LIKE '%000' ORDER BY ParcelID\"\n",
    "\n",
    "#Create DataFrame from PrideMain Table\n",
    "pCsv = psProcessing / 'PrideMain.csv'\n",
    "df_Pride = pd.read_sql(sql=q, con=cnxn)\n",
    "df_Pride.to_csv(pCsv, index=False)\n",
    "print('Created Pride Main DataFrame')\n",
    "# df_Pride.shape"
   ]
  },
  {
   "source": [
    "### Curate the Pride Main Table "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add dash and 01 to ParcelID for merge\n",
    "print('Changing Pride Main Table')\n",
    "def pForm(field):\n",
    "    return field.strip().replace(' ', '-')\n",
    "\n",
    "df_Pride_a = df_Pride\n",
    "df_Pride_a.rename(columns=lambda x: x.strip().upper(), inplace=True)\n",
    "df_Pride_a.PARCELID= df_Pride_a.PARCELID.apply(pForm)\n",
    "df_Pride_a.PARCELID = df_Pride_a.PARCELID.apply(lambda x: str(x + '01'))\n",
    "# df_Pride_a.drop('ParcelID', axis=1, inplace=True)\n",
    "df_Pride_a.fillna(' ', axis = 1, inplace = True)\n",
    "print('Finished Changing Pride Main Table')"
   ]
  },
  {
   "source": [
    "### Copy and Create DataFrame from AS400 All Parcel Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame from AS400 Data\n",
    "d = netDir / 'ParcelWithSuffix' / 'ENK003NW.csv'\n",
    "# d = r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben\\Permits\\ENK003NW.csv'\n",
    "aSD = psReview / 'AS400.csv'\n",
    "print(f\"Copying {d.name} to {aSD}\")\n",
    "copy2(d, aSD)\n",
    "print(f\"Finished Copying {aSD.stem} to {aSD.parent}\")\n",
    "as_df = pd.read_csv(d)\n",
    "# as_df.shape\n",
    "print('Created AS400 DataFrame')"
   ]
  },
  {
   "source": [
    "### Remove Whitespace from headers in AS400 Data. Renamed PROPERTY DESC (Not the abbreviations) to PROPERTY USE DESC"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fix Column names from AS400 import\n",
    "\n",
    "# cFix = as_df.columns.tolist()\n",
    "# pList = [f.strip() for f in cFix]\n",
    "# print(pList)\n",
    "\n",
    "# cpFix = dict(zip(cFix, pList))\n",
    "# print(cpFix)\n",
    "\n",
    "# as_df = as_df.rename(columns=cpFix)\n",
    "# as_df\n",
    "\n",
    "as_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "# print(as_df.columns)"
   ]
  },
  {
   "source": [
    "### Remove Parcels with Suffix from merge of Pride Main and AS400 Parcel Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "as_df = as_df[~as_df.PARCELID.str.endswith('-00001', na=False)]\n",
    "# as_df.shape"
   ]
  },
  {
   "source": [
    "### Merge Pride Main (df_Pride_a) with AS400 Parcel Data (as_df) and extract the differences for Review later. Not for production. Use to keep track of the difference between AP5 and AS400."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<b> Right_Only has the values only in the AS400 Parcel Data, left_only has the values only in Pride Main Table, and both is a successful match. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the difference between Pride Main and AS400 All Parcels Included.\n",
    "#Possibly create for loop to create the csvs, copy from sheriff notebook\n",
    "map_df = df_Pride_a.merge(as_df,on='PARCELID', how='outer', indicator=True)\n",
    "print(\"Created map_df from Pride Table and AS400 Parcel Data.\")\n",
    "\n",
    "map_df[map_df['_merge'] == 'both'].to_csv(psReview / 'Pride_AS_Both.csv')\n",
    "map_df[map_df['_merge'] == 'right_only'].to_csv(psReview / 'Pride_AS_Right.csv')\n",
    "map_df[map_df['_merge'] == 'left_only'].to_csv(psReview / 'Pride_AS_Left.csv')\n",
    "print(\"Exported all the DFs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the amount of values in DataFrame to check if the data is matching\n",
    "pcValues = len(map_df[map_df._merge == 'both'])\n",
    "# print(pcValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the data from the Pride Main Table.\n",
    "map_df_a = map_df.loc[(map_df._merge == 'left_only') | (map_df._merge == 'both'), :]\n",
    "del map_df_a['_merge']\n",
    "pmValues = len(map_df)\n",
    "# print(pmValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove whitespace from Formatted Location in preparation for merge with addressing data\n",
    "map_df_a['FLOCATION'] = map_df_a['FORMATTEDLOCATION'].apply(lambda x: \" \".join(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove Columns from df_Pride that are not needed.\n",
    "# pClist = df_Pride.columns.tolist()\n",
    "aClist = as_df.columns.tolist()\n",
    "kCol = ['PARCELID']\n",
    "# print(aClist)\n",
    "map_df_a.drop(columns=[x for x in aClist if x not in kCol], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_df_a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(mo_df[mo_df._merge == 'both']))\n",
    "# print(df_Pride_a.columns)\n",
    "# del map_df"
   ]
  },
  {
   "source": [
    "### Create DataFrame from Addressing Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create Addressing DF from Addressing Feature Class\n",
    "addFC = [f for f in cParcels if f.name.startswith('Addressing') == True][0]\n",
    "# print(str(addFC))\n",
    "# lFC = None\n",
    "# if addFC = True:\n",
    "#     lFC = \n",
    "df_add = features.GeoAccessor.from_featureclass(addFC)\n",
    "# df_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Check the columns for the addressing DF\n",
    "col_df_add = [f for f in df_add.columns]\n",
    "# print(col_df_add)\n",
    "\n",
    "# df_add = df_add.loc[:, ['FullAddr','SHAPE']]\n",
    "# addC = list(df_add.columns.values)\n",
    "# # print(addC)\n",
    "# keep = ['FullAddr', 'SHAPE']\n",
    "# for x in addC[:]:\n",
    "#     if x not in keep:\n",
    "# #         print(x)\n",
    "#         addC.remove(x)\n",
    "# print(addC)\n",
    "    "
   ]
  },
  {
   "source": [
    "<b>Edit Address DataFrame to remove nulls, correct misnamed Roads, and to remove whitespace</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(len(df_Pride.iloc[0]))\n",
    "# print(len(df_Pride.loc[0, :]))\n",
    "# print(len(df_Pride.loc[0, ['FormattedLocation']]))\n",
    "# df_Pride\n",
    "# df_Pride.loc[0, ['FormattedLocation']].values()\n",
    "# print(len(df_Pride.iloc[0, -1]))\n",
    "# df_Pride.iloc[0, -1]\n",
    "\n",
    "# df_Pride['H'] = df_Pride\n",
    "# df_Pride.drop(columns=['G'], inplace=True)\n",
    "# df_Pride\n",
    "# df_Pride.loc[df_Pride['FormattedLocation'].str.startswith('45 POM')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_dropper = df_add.dropna(axis=0, subset=['FullAddr', 'Roadname'])\n",
    "# df_dropper.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_add =df_dropper\n",
    "# del df_dropper\n",
    "# df_add"
   ]
  },
  {
   "source": [
    "Create new address field using the MSAG columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add['STS'].replace('None', \" \")\n",
    "# df_add.loc[df_add.STS == None, :]\n",
    "# df_add.isnull().sum()\n",
    "df_add['STS'].fillna(np.nan, inplace= True)\n",
    "df_add['STS'].replace(' ', '', inplace=True)\n",
    "df_add['STS'].replace(np.nan, '', inplace=True)\n",
    "# df_add['STS'].unique()\n",
    "# df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aType = df_add.STS.unique()\n",
    "# print(sType)\n",
    "for d in aType:\n",
    "    if d.upper() == 'ROAD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'RD').strip())\n",
    "    elif d.upper() == 'DRIVE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "    elif d.upper() == 'BOULEVARD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "    elif d.upper() == 'LANE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "    elif d.upper() == 'COURT':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "    elif d.upper() == 'CIRCLE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "    elif d.upper() == 'STREET':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "    elif d.upper() == 'HIGHWAY':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "    elif d.upper() == 'AVENUE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "    elif d.upper() == 'PLACE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "    else:\n",
    "        df_add.STS.str.strip()\n",
    "# print(df_add.STS.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.fillna(np.nan, inplace= True)\n",
    "# df_add.replace(' ', '', inplace=True)\n",
    "df_add.replace(np.nan, '', inplace=True)\n",
    "# print(df_add.columns)\n",
    "# print(df_add.STS.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.SAN = df_add.SAN.astype('str',errors='ignore')\n",
    "df_add.SAN = df_add.SAN.apply(lambda x: x.split('.')[0])\n",
    "df_add['FAddress'] = df_add[['SAN', 'PRD', 'STN', 'STS', 'POD']].values.tolist()\n",
    "# df_add.FAddress.values\n",
    "df_add['FooAddress'] = df_add['FAddress'].apply(' '.join)\n",
    "df_add['FoAddress'] = df_add['FooAddress'].apply(lambda x: ' '.join(x.split()))\n",
    "# df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.FAddress = df_add.FoAddress\n",
    "kColumns = ['FAddress', 'SHAPE']\n",
    "# print([x for x in kColumns if x not in kColumns])\n",
    "df_add.drop(columns=[x for x in df_add if x not in kColumns], inplace=True)\n",
    "# df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Nulls in FullAddr Field\n",
    "df_add[df_add['FAddress'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for empty values in FullAddr Field\n",
    "# bAddress = pReview / f'BlankAddress_{dStr}.csv'\n",
    "# df_add[df_add.FullAddr == ' '].to_csv(bAddress)\n",
    "df_add[df_add.FAddress == ' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Values that are not null\n",
    "df_add.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_a = df_add"
   ]
  },
  {
   "source": [
    "Only use if MSAG columns do not contain data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove Nulls and blanks from FullAddr field\n",
    "df_add_a = df_add.loc[((df_add.FullAddr != ' ') & (df_add.FullAddr.notnull()))]\n",
    "df_add_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove whitespace from FullAddr\n",
    "df_add_a['FAddr'] = df_add_a['FullAddr'].apply(lambda x: \" \".join(x.split()))\n",
    "df_add.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addchange(field):\n",
    "    cNames = ['DRIVE', 'CIRCLE', 'ROAD', 'LANE']\n",
    "    if field.endswith('DRIVE'):\n",
    "        return field.replace('DRIVE', 'DR')\n",
    "    elif field.endswith('CIRCLE'):\n",
    "        return field.replace('CIRCLE', 'CIR')\n",
    "    elif field.endswith('ROAD'):\n",
    "        return field.replace('ROAD', 'RD')\n",
    "    elif field.endswith('LANE'):\n",
    "        return field.replace('LANE', 'LN')\n",
    "    else:\n",
    "        return field.strip()\n",
    "df_add_a['Address'] = df_add_a['FAddr'].apply(addchange)\n",
    "# df_add\n",
    "# df_add.loc[df_add['FullAddr'].str.contains('Round', na=False)]\n",
    "# df_Pride_Add.loc[df_Pride_Add['FullAddr'].str.contains('Round', na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_a.drop(columns=['FAddr', 'FullAddr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_add_a = df_add_a[['Address', 'SHAPE']]\n",
    "df_add_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_add.loc[df_add['FullAddr'].str.startswith('45 POM')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# def doit(field):\n",
    "#     if len(field.split(' ')) == 3:\n",
    "#         return f'{field.split()[0]} {field.split()[1]} {field.split()[2]}'\n",
    "#     else:\n",
    "#         return field\n",
    "\n",
    "# df_Pride['F'] = df_Pride['FormattedLocation'].apply(doit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# do = ['10 POM RUN', '10 10 10', '10 10 10 10']\n",
    "# for x in do:\n",
    "#     if len(x.split()) == 3:\n",
    "#         print(x.split()[0])\n",
    "#         print(x)\n",
    "#     print(len(x.split()))\n",
    "# df_Pride\n",
    "# df_Pride.loc[df_Pride['F'] == '10 POM RUN', :]\n",
    "# del df_Pride['_merge']\n",
    "# del geo_df\n",
    "# df_Pride.loc[:, ['FormattedLocation', 'EXPROP']]\n",
    "# df_Pride.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_Pride.loc[df_Pride['FormattedLocation'] == '45 POM RUN', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_add.loc[df_add['FullAddr'] == '10 COUNCIL DR', :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# q = geo_df['FullAddr'] == geo_df['FormattedLocation']\n",
    "# df1 = geo_df[q][['FullAddr', 'FormattedLocation']]\n",
    "# df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geo_df.loc[geo_df['FullAddr'] == '11 COUNCIL DR', :]"
   ]
  },
  {
   "source": [
    "### Create Points from Parcel Suffix Feature Class and create DataFrame from the points FC."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Points out of polygons in Suffix_Parcels\n",
    "sufPar = [f for f in cParcels if f.name.startswith(\"Suffix\")][0]\n",
    "# print(sufPar.parent)\n",
    "# print(cParcels)\n",
    "sPoints = locGDB / 'sPoints'\n",
    "# print(sPoints)\n",
    "if arc.Exists(f'{sPoints}'):\n",
    "    print(f'{sPoints.name} already exists')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(sufPar), f'{sPoints}', \"INSIDE\")\n",
    "    print(f'{sPoints.name} has been created in {locGDB}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DF from Suffix_Parcels\n",
    "# sFields = [f.name for f in arc.ListFields(sPoints)]\n",
    "# print(sFields)\n",
    "sp_df = features.GeoAccessor.from_featureclass(sPoints)\n",
    "sp_df.drop(columns=['OBJECTID', 'ORIG_FID'], inplace=True)\n",
    "sp_df.rename(columns={'Name': 'Development', 'Lot':'DevLot'}, inplace=True)\n",
    "sp_df.rename(columns=lambda x: x.strip().upper(), inplace=True)\n",
    "print('Suffix Parcels DataFrame created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sp_df.columns)\n",
    "# print(map_df_a.columns)\n",
    "# print(df_add.columns)\n",
    "# print(df_Pride_m.columns)"
   ]
  },
  {
   "source": [
    "### Merge Pride Tables (Pride Main and AS400 Parcel Data) with Parcels with Suffix Data(sp_df). Export them for Review. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<b> Left_Only = Values in map_df_a and right_only = Values in sp_df. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Pride Table (PrideMain and AS400 Data) and Parcels with Suffix Data (sp_df).\n",
    "df_Pride_m = map_df_a.merge(sp_df, on='PARCELID', how='outer', indicator=True)\n",
    "print(\"Created df_Pride_m from Pride Table and Addressing Data.\")\n",
    "# df_Pride_m.shape\n",
    "\n",
    "#export the csvs with both being the base data\n",
    "df_Pride_m[df_Pride_m['_merge'] == 'left_only'].to_csv(psReview / 'Pride_Left.csv', index=False)\n",
    "df_Pride_m[df_Pride_m['_merge'] == 'right_only'].to_csv(psReview / 'SP_Right.csv', index=False)\n",
    "df_Pride_m[df_Pride_m['_merge'] == 'both'].to_csv(psReview / 'Pride_Suf_Both.csv', index=False)\n",
    "print('Exported Pride Table and Suffix Data')"
   ]
  },
  {
   "source": [
    "df_Pride_mb is values that matched in the merge betwee Pride Table and Parcels With Suffix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_mb = df_Pride_m[df_Pride_m._merge == 'both'].copy(deep=True)\n",
    "p_suf_b = len(df_Pride_mb)\n",
    "# print(p_suf_b)"
   ]
  },
  {
   "source": [
    "### Take values not matched from df_Pride_m, which are from original Pride Main and AS400 Parcel Data merge, and try to match them with their address to df_add_a. Export for Review."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df_Pride_L = Values from df_Pride_m(merge between Pride Table and Suffix Parcels) that didn't match."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_L = df_Pride_m[df_Pride_m._merge == 'left_only'].copy(deep=True)\n",
    "p_suf_l = len(df_Pride_L)\n",
    "# print(p_suf_l)"
   ]
  },
  {
   "source": [
    "Remove Unit from the FLocation Column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(field):\n",
    "    a = field.split(', ')\n",
    "    if a[-1].lower().startswith('unit'):\n",
    "        return a.pop(0)\n",
    "    else:\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_L.FLOCATION = df_Pride_L.FLOCATION.apply(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove _merge column\n",
    "# del df_Pride_m['_merge']\n",
    "#Unmatched Values from Pride Table (Pride and AS400 Data) merged with Addressing Data\n",
    "df_Pride_L.drop(columns=['_merge', 'SHAPE'] , inplace=True)\n",
    "df_Pride_all = df_Pride_L.merge(df_add_a, left_on='FLOCATION', right_on='FAddress', how='outer', indicator=True)\n",
    "# print(df_Pride_all.shape)\n",
    "print('Created df_Pride_all from Pride Table and Addressing Data.')"
   ]
  },
  {
   "source": [
    "<b> Left_Only = Values in df_Pride_all. Right_Only = Values in df_Pride_L."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csvs to see what needs to be fixed both needs to be appended to df_Pride_m\n",
    "df_Pride_all[df_Pride_all['_merge'] == 'left_only'].to_csv(psReview / 'ASP_Add_Left.csv', index=False)\n",
    "df_Pride_all[df_Pride_all['_merge'] == 'both'].to_csv(psReview / 'ASP_Add_Both.csv', index=False)\n",
    "df_Pride_all[df_Pride_all['_merge'] == 'right_only'].to_csv(psReview / 'ASP_Add_Right.csv', index=False)\n",
    "print('Exported Pride Table and Address Table.')"
   ]
  },
  {
   "source": [
    "<b> df_Pride_ab is values that matched between Pride, Suffix Merge and Addressing Data. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_ab = df_Pride_all[df_Pride_all._merge == 'both'].copy(deep=True)\n",
    "p_add_b = len(df_Pride_ab)\n",
    "# print(p_add_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_al = df_Pride_all[df_Pride_all._merge == 'left_only'].copy(deep=True)\n",
    "p_add_l = len(df_Pride_al)\n",
    "# print(p_add_l)"
   ]
  },
  {
   "source": [
    "loc_df are values that don't start with a number. geo_df are values that start with a number to make it easier to geocode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = df_Pride_al[~df_Pride_al.FLOCATION.str.contains('^[0-9]+')].copy(deep=True)\n",
    "geo_df = df_Pride_al[df_Pride_al.FLOCATION.str.contains('^[0-9]+')].copy(deep=True)"
   ]
  },
  {
   "source": [
    "<b> Added PARENT Column to match with Parcel Fabric DataFrame, edited addressing to match addressing DataFrame, updated the columns names, created column (OriginalAdd) to maintain the non-edited address. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(field):\n",
    "    a = field.split('-')\n",
    "    return f'{a[0]}-{a[1]}-{a[2]}-{a[3]}-{a[4]}-00001'\n",
    "\n",
    "loc_df['PARENT']= loc_df.PARCELID.apply(change)\n",
    "geo_df['PARENT'] = geo_df.PARCELID.apply(change)"
   ]
  },
  {
   "source": [
    "Clear up column names in loc_df and geo_df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.dropna(axis=1, how='all', inplace=True)\n",
    "loc_df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.dropna(axis=1, how='all', inplace=True)\n",
    "geo_df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "source": [
    "### CSV with manual edits to get address data into correct format. Created from ASP_Add_Left output. Do not use for automation since the parcels could change in the future."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df from edits CSV\n",
    "pDir = netDir / 'ParcelWithSuffix' / 'Edits_ASP_Add_Left.csv'\n",
    "cLocal = psReview / pDir.name\n",
    "print(f'Copying {pDir.name} to {cLocal.parent}')\n",
    "copy2(pDir, cLocal)\n",
    "print(f'Finished Copying {cLocal.name} to {cLocal.parent}')\n",
    "df_Pride_edits = pd.read_csv(cLocal)\n",
    "df_Pride_edits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_edits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_edits = df_Pride_edits.rename(columns={'Unnamed: 7' : 'OriginalAdd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_add_a.columns)\n",
    "print(df_Pride_edits.columns)"
   ]
  },
  {
   "source": [
    "<b> Merge Addressing with edits </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge addressing with df_Pride_edits\n",
    "df_Pride_Add = df_Pride_edits.merge(df_add_a, left_on='UPPER_ADDR', right_on='Address', how='outer', indicator=True)\n",
    "# df_Pride_Add"
   ]
  },
  {
   "source": [
    "Left_Only = Values from df_Pride_edits. Right_Only = Values from df_add"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_Add[df_Pride_Add['_merge'] == 'left_only'].to_csv(psReview / 'Ed_Add_Left.csv', index=False)\n",
    "df_Pride_Add[df_Pride_Add['_merge'] == 'both'].to_csv(psReview / 'Ed_Add_Both.csv', index=False)\n",
    "df_Pride_Add[df_Pride_Add['_merge'] == 'right_only'].to_csv(psReview / 'Ed_Add_Right.csv', index=False)\n",
    "print('Exported Pride Table Edits and Address Table.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_Add.loc[df_Pride_Add._merge == 'left_only']\n",
    "#  & df_Pride_Add.FullAddr.str.contains('1679')\n",
    "df_Pride_AddL = df_Pride_Add.loc[df_Pride_Add._merge == 'left_only']\n",
    "df_Pride_AddB = df_Pride_Add.loc[df_Pride_Add._merge == 'both']\n",
    "print(df_Pride_AddL.shape)\n",
    "print(df_Pride_AddB.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add.loc[(df_add.STN.str.contains('KENTWOOD', na=False)) & (df_add['SAN'].astype(str).str.contains('23', na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Pride_AddL.drop(columns='_merge', inplace=True)\n",
    "# df_Pride_AddL.drop(columns='SHAPE', inplace=True)\n",
    "df_Pride_AddL.columns"
   ]
  },
  {
   "source": [
    "### Create Points from Parcel Fabric for geospatial information for Addresses and Parcels with Suffixes that don't match"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to not rely on arc.env.workspace for outputs.\n",
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "# print(pfFC)\n",
    "pfPoints = locGDB / f'pfPoints_{dStr}'\n",
    "# print(pfPoints)\n",
    "if arc.Exists(f'{pfPoints}'):\n",
    "    print(f'{pfPoints.name} already exists.')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(pfFC), f'{pfPoints}', \"INSIDE\")\n",
    "    print(f'{pfPoints.name} has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from pfPoints FC\n",
    "pfP_df = features.GeoAccessor.from_featureclass(pfPoints, fields=['Name'])\n",
    "print('Parcel Fabric Points DataFrame created.')\n",
    "# pfP_df.shape\n",
    "# pfP_df.columns\n",
    "\n",
    "#Use if need to edit field names in pfPoints FC\n",
    "# pfFields = [f.name for f in arc.ListFields(pfPoints)]\n",
    "# print(pfFields)"
   ]
  },
  {
   "source": [
    "<b> Attempt to geocode the final addresses. Can use OSM Nominatim, ESRI AGOL (uses credits from org), FirstMap Locator on their REST Server, or KC batch geocoder on REST Server.</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del Add_Par_df['_merge']\n",
    "# Add_Par_df = df_Pride_AddL.merge(pfP_df, how='inner', left_on='Fu', right_on='Name')\n",
    "# Add_Par_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add_Par_df[Add_Par_df.Name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Pride_AddL[df_Pride_AddL.Fu.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addPardf = pReview / 'AddParDF.csv'\n",
    "# Add_Par_df.to_csv(addPardf, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from arcgis.gis.server import Server\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "pURL = os.getenv('PORTAL_SITE')\n",
    "aURL = os.getenv('AGOL_SITE')\n",
    "sURL = os.getenv('STAGE_SITE')\n",
    "sUser = os.getenv('STAGE_USERNAME')\n",
    "sPass = os.getenv('STAGE_PASSWORD')\n",
    "aUser = os.getenv('AGOL_USERNAME')\n",
    "aPass = os.getenv('AGOL_PASSWORD')\n",
    "pUser = os.getenv('ESRI_USERNAME')\n",
    "pPass = os.getenv('ESRI_PASSWORD')\n",
    "\n",
    "#Connect to Enterprise GIS\n",
    "gisE = GIS(url=pURL, username=pUser, password=pPass)\n",
    "\n",
    "#Connect to AGOL\n",
    "gisA = GIS(url=aURL, username=aUser, password=aPass, set_active=False)\n",
    "\n",
    "#Connect to Staging. Does not work for right now. Found a solution: https://community.esri.com/t5/developers-questions/error-connecting-to-arcgis-server-with-arcgis-python-api/m-p/869042?commentID=896496#comment-896496\n",
    "\n",
    "# gisT = GIS(url=sURL, username=sUser, password=sPass, set_active=False, verify_cert=False)\n",
    "# from arcgis.gis.server import Server\n",
    "# sURL = 'https://ESRISTAGE:6443'\n",
    "# gisT = Server(url=f'{sURL}/ed8fb7ce-e99d-4eb2-af78-b7ef11e3c2c4/admin',token_url=f'{sURL}/ed8fb7ce-e99d-4eb2-af78-b7ef11e3c2c4/tokens/generateToken', username=sUser, password=sPass, verify_cert=False, set_active=False)\n",
    "\n",
    "#This method for connecting to staging server does work. Must use Server Module for now\n",
    "gisT = Server(url=f'{sURL}/arcgis/admin', username=sUser, password=sPass, verify_cert=False)\n",
    "\n",
    "# gisT = GIS(url=f'{sURL}/arcgis/admin', username=sUser, password=sPass, verify_cert=False)\n",
    "\n",
    "\n",
    "print(gisA)\n",
    "print(gisE)\n",
    "print(gisT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arcgis.gis.server import ServerManager\n",
    "\n",
    "# gisT.datastores.add_database()\n",
    "\n",
    "# kServ = gisE.admin.servers.list()[0]\n",
    "# kServ.datastores.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addz(field, field1):\n",
    "    if field1 != ' ':\n",
    "        return f'{field}, {field1}'\n",
    "    else:\n",
    "        return field\n",
    "geo_df['ZLOCATION'] = geo_df.apply(lambda x: addz(x['FLOCATION'], x['ZIPCODE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geocoding from_df is very buggy, changed the _accessor.py located at C:\\Users\\MKinnaman\\Anaconda3\\envs\\arcNew\n",
    "#\\Lib\\site-packages\\arcgis\\features\\geo. Added Line 2247 to 2250 and Line 2261.\n",
    "\n",
    "from arcgis import geocoding\n",
    "from arcgis.geocoding import Geocoder\n",
    "# from arcgis.features import GeoAccessor\n",
    "\n",
    "locFM = 'https://enterprise.firstmap.delaware.gov/arcgis/rest/services/Location/Delaware_FirstMap_Locator/GeocodeServer'\n",
    "geoc = Geocoder(locFM)\n",
    "\n",
    "# print(geoc.properties.locatorProperties.MaxBatchSize)\n",
    "\n",
    "# def addST(field):\n",
    "#     return field + ', DOVER, DE'\n",
    "# df_Pride_AddL['GAddress'] = df_Pride_AddL.UPPER_ADDR.apply(addST)\n",
    "# df_Pride_AddL\n",
    "\n",
    "\n",
    "# if len(geo_df.ZLOCATION) <= 499:\n",
    "#     aList = geo_df.ZLOCATION.values.tolist()\n",
    "# else:\n",
    "#     clist = [aList[i:i + 499] for i in range(0, len(aList), 499)]\n",
    "\n",
    "# add_geom = []\n",
    "# geo_b = {}\n",
    "# if aList is not None:\n",
    "#     geo = geocoding.batch_geocode(aList, out_sr=26957, geocoder= geoc)\n",
    "#     add_geom.append(geo)\n",
    "# else:\n",
    "#     j = len(clist)\n",
    "#     for i in range(0, j):\n",
    "#       geo = geocoding.batch_geocode(clist[i], )\n",
    "\n",
    "geo_B = features.GeoAccessor.from_df(geo_df, address_column='ZLOCATION', geocoder=geoc, sr=26957)\n",
    "geo_B"
   ]
  },
  {
   "source": [
    "Create CSV of geocoded addresses."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_add = psReview / 'Geo_Add.csv'\n",
    "geo_add_df = geo_B[geo_B.SHAPE.notnull()].copy(deep=True)\n",
    "geo_add_df.to_csv(geo_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_match = len(geo_add_df)\n",
    "# print(geo_match)"
   ]
  },
  {
   "source": [
    "Get the values that were not matched after geocoding and merge them with the Parcel ID from Parcel Fabric Points (pfP_df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_b_null = geo_B[geo_B.SHAPE.isnull()].copy(deep=True)\n",
    "del geo_b_null['SHAPE']\n",
    "geo_pf = geo_b_null.merge(pfP_df, left_on='PARENT', right_on='Name', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = psReview / 'MissingParcels.csv'\n",
    "geo_pf.loc[geo_pf._merge == 'left_only', :].to_csv(dl, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_pf_df = geo_pf[geo_pf._merge == 'both'].copy(deep=True)\n",
    "geo_pf_b = len(geo_pf_df)\n",
    "# print(geo_pf_b)"
   ]
  },
  {
   "source": [
    "Merge parcels with an address with Parcel Fabric Points."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pl = psReview / 'MissingParcels_2.csv'\n",
    "loc_pf = loc_df.merge(pfP_df, left_on='PARENT', right_on='Name', how='outer', indicator=True)\n",
    "loc_pf.loc[loc_pf._merge == 'left_only', :].to_csv(pl, index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_pf_df = loc_pf.loc[loc_pf._merge == 'both', :]\n",
    "loc_pf_b = len(loc_pf_df)\n",
    "# print(loc_pf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [p_suf_b, p_add_b, geo_pf_b, loc_pf_b, geo_match]\n",
    "sum(ls)"
   ]
  },
  {
   "source": [
    "<b>Concat all the DFs into one. All of the DFs need to have the same columns. df_Pride_mb, df_pride_ab, geo_pf_df, loc_pf_df, and geo_add_df</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geo_pf_df.columns)\n",
    "print(df_Pride_ab.columns)\n",
    "print(df_Pride_mb.columns)\n",
    "print(loc_pf_df.columns)\n",
    "print(geo_add_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_pf_df.drop(columns=['_merge','Name'], inplace=True)\n",
    "gList = geo_pf_df.columns.tolist()\n",
    "# gList.remove('_merge')\n",
    "# kCol = ['PARCELID']\n",
    "# print(aClist)\n",
    "# df_Pride_ab.drop(columns=[x for x not in gList], inplace=True)\n",
    "# map_df_a.columns\n",
    "df_Pride_ab.drop(columns=[x for x in df_Pride_ab.columns if x not in gList], inplace=True)\n",
    "df_Pride_mb.drop(columns=[x for x in df_Pride_mb.columns if x not in gList], inplace=True)\n",
    "loc_pf_df.drop(columns=[x for x in loc_pf_df.columns if x not in gList], inplace=True)\n",
    "geo_add_df.drop(columns=[x for x in geo_add_df.columns if x not in gList], inplace=True)"
   ]
  },
  {
   "source": [
    "Add PARENT column to all of the DFs for use with MGO Model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_ab['PARENT'] = df_Pride_ab.PARCELID.apply(change)\n",
    "df_Pride_mb['PARENT'] = df_Pride_mb.PARCELID.apply(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [geo_pf_df, df_Pride_ab, df_Pride_mb, loc_pf_df, geo_add_df]\n",
    "df_Append_all = pd.concat(frames, axis=0)\n",
    "cDF = psReview / 'ParcelWithSuffix.csv'\n",
    "df_Append_all.to_csv(cDF, index=False)"
   ]
  },
  {
   "source": [
    "Edit the column names for clarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oCols = [f.lower() for f in df_Append_all.columns]\n",
    "rCols = {'PARCELID': 'Name', 'FLOCATION' : 'Address'}\n",
    "dCols = ['BILLINGADDRESS', 'BILLINGADDRESS2', 'ZIPCODE', 'FORMATTEDLOCATION', 'ZLOCATION']\n",
    "df_Append_all.drop(columns=dCols, inplace=True)\n",
    "df_Append_all.rename(columns=rCols, inplace=True)\n",
    "# df_Append_all.rename(columns=lambda x: x.title(), inplace=True)\n",
    "# df_Append_all.rename(columns=lambda x: x.upper(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Append_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Append_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "source": [
    "Testing for bugs in arcgis package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outc = psReview / 'all.csv'\n",
    "# df_Append_all.to_csv(outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(geo_pf_df.spatial.sr)\n",
    "# print(df_Pride_ab.spatial.sr)\n",
    "# print(df_Pride_mb.spatial.sr)\n",
    "# print(loc_pf_df.spatial.sr)\n",
    "# print(geo_add_df.spatial.sr)\n",
    "# print(df_Append_all.spatial.sr)\n",
    "# print(pfP_df.spatial.sr)\n",
    "# print(geo_B.spatial.sr)\n",
    "# outFC = locGDB / f'geo_Test'\n",
    "# geo_pf_df.spatial.to_featureclass(f'{outFC}', sanitize_columns=False)\n",
    "# from uuid import uuid4\n",
    "# import random\n",
    "# import string\n",
    "# for x in frames:\n",
    "#     outFC = locGDB / f'{random.choice(string.ascii_lowercase)}'\n",
    "#     a = pd.DataFrame(x)\n",
    "#     a.spatial.to_featureclass(location=f'{outFC}', sanitize_columns=False)\n",
    "\n",
    "# frames = [geo_pf_df, df_Pride_ab, df_Pride_mb, loc_pf_df, geo_add_df]"
   ]
  },
  {
   "source": [
    "Output the DataFrame to the geodatabase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFC = locGDB / f'ParcelWithSuffix_{dStr}'\n",
    "df_Append_all.spatial.to_featureclass(f'{outFC}', sanitize_columns=False)\n",
    "print(f'Copied {outFC.name} to {locGDB}')"
   ]
  },
  {
   "source": [
    "Publish the DataFrame to Portal for use with online mapping."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Append_all.spatial.to_featurelayer('ParcelWithSuffix', gis= gisE)\n",
    "print('Added ParcelWithSuffix to Portal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd075dad33b15f83a112a28040a32ba03061e24a62911504b8f236300170e604621",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}