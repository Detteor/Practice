{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Permits, Violations, and TaxParcels File with MGO Data and AS 400 Data.\n",
    "\n"
   ]
  },
  {
   "source": [
    "<b> TO DO: </b>\n",
    "<ul> <li><del>get values out of the year column to create multiple DFs using those years.</del></li>\n",
    "<li>also can create a dictionary with those years in there.</li>\n",
    "<li>use dask to create a lot of DFs for processing.</li>\n",
    "<li><del>Add GeoSpatial Data to All Permits for Historical Permits.</del></li>\n",
    "<li><del>Remove Permits from the DB and upload the new Permits from MGO and AS400 Data.</del></li>\n",
    "<li>Add to Daily Script.</li>\n",
    "<li>Add YEAR column to AllPermit Feature Class on ESRIDB.</li>\n",
    "</ul>\n",
    "\n",
    "<b>Notes:</b>\n",
    "<ul> \n",
    "<li>Permits table on ESRIDB is the dump created from the SQL job \"Update Permit Table\", this table is very filtered by AS400. This table is used for some old scripts that are going to be deleted. </li></br>\n",
    "<li>All_Permits on ESRIDB is the dump from AS400. Should not be updated anymore due to Permits no longer going through AS400.Also, contains all the permits in history. </li></br>\n",
    "<li>HPermit is the processed version of All_Permits bot h are just tables and do not contain any spatial information. </li></br>\n",
    "<li>Historical Permit is the permits from AS400 with some spatial information. By checking the spatial reference you can filter the table.</li></br>\n",
    "<li>AllPermit is the matching Parcel #'s from AS400 permits (using Parcel Fabric and ParcelWithSuffix parcels) </li></br>\n",
    "<li>Current Permit is the permits from AS400 and MGO that are the most recent and  contain spatial information. Does not include Demo Permits and removes the duplicates based on Parcel # </li></br>\n",
    "<li>The same naming convention applies to the Demo Permits.</li>\n",
    "\n",
    "</ul>\n",
    " \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Declare Global Variables for geoprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use arcNew environment with this notebook.\n",
    "#Current Location: C:\\ProgramData\\Anaconda3\\envs\\arcNew\\python.exe\n",
    "import arcpy as arc\n",
    "import os\n",
    "from shutil import copy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from arcgis import features\n",
    "from datetime import datetime as dt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "now = dt.now()\n",
    "mStr = now.strftime('%m%Y')\n",
    "dStr = now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'\n",
    "# print(netDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Only use with notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For use when in production."
   ]
  },
  {
   "source": [
    "### Create Folders for Permits Review Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Folders for Permits Data\n",
    "pFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "pProcessing = pFolder / 'Permits' / f'{dStr}'\n",
    "if pProcessing.exists() == True:\n",
    "    print(f'{pProcessing} already exist.')\n",
    "else:\n",
    "    pProcessing.mkdir(parents=True)\n",
    "    print(f'Created {pProcessing}.')\n",
    "\n",
    "pFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "pReview = pFR / 'Permits' / f'{dStr}'\n",
    "if pReview.exists() == True:\n",
    "    print(f'{pReview} already exist')\n",
    "else:\n",
    "    pReview.mkdir(parents=True)\n",
    "    print(f'Created {pReview}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New File GeoDatabase and corresponding Feature Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "sr = arc.Describe(f'{iE}').spatialReference\n",
    "outGDB = gisPath / pFolder / f'Data_{mStr}.gdb'\n",
    "# dsA = gisPath / pFolder / f'{outGDB}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dStr}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{pFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Parcel Fabric DataFrame and Points FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Does not work with Parcel Fabric due to bug with 10.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbAdd = 'PROD.ADDRESSINGADMIN.Addressing'\n",
    "dbSufP = 'PROD.GISADMIN.Suffix_Parcels'\n",
    "dbMPTab = 'PROD.gisadmin.MGOPermits'\n",
    "dbAPTab = 'PROD.gisadmin.All_Permits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#create paths to the data instead of using env.workspace makes it so that I can use arc Walk\n",
    "arc.env.workspace = f'{iE.parent}'\n",
    "\n",
    "fcList = [dbAdd, dbSufP]\n",
    "tList = [dbMPTab, dbAPTab]\n",
    "cParcels = []\n",
    "cTables = []\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in fcList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}'\n",
    "            if arc.Exists(f'{locGDB / b}'):\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.FeatureClassToFeatureClass_conversion(f, f'{locGDB}', f'{b}')\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in tList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}' #Tables need to have a date string since they won't be under the dataset\n",
    "            # c = locGDB.parent\n",
    "            if arc.Exists(f'{outGDB / b}'):\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.TableToTable_conversion(f, f'{outGDB}', f'{b}')\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "# print(cParcels)\n",
    "# print(cTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Parcel Fabric to local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#currently using env.workspace to move parcelfabric needs to be changed to use path\n",
    "pfParcels = 'PROD.MAPPINGADMIN.ParcelFabric_Parcels'\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "a = pfParcels.split('.')[-1]\n",
    "b = f'{a}_{dStr}'\n",
    "# print(f'{iEPath[1]}')\n",
    "if arc.Exists(f'{locGDB / b}'):\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'{b} already exists')\n",
    "else:\n",
    "    print(f'Copying Parcel Fabric to {locGDB}')\n",
    "    #print(a)\n",
    "    # g = f'{f}'\n",
    "    #print(g)\n",
    "    arc.FeatureClassToFeatureClass_conversion(pfParcels, f'{locGDB}', b, where_clause=exp)\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Create points out of Parcel Fabric to be used with Permits.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create Points out of polygons in ParcelFabric\n",
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "# print(pfFC)\n",
    "pfPoints = locGDB / f'pfPoints_{dStr}'\n",
    "# print(pfPoints)\n",
    "if arc.Exists(f'{pfPoints}'):\n",
    "    print(f'{pfPoints.name} already exists.')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(pfFC), f'{pfPoints}', \"INSIDE\")\n",
    "    print(f'{pfPoints.name} has been created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get List of Fields from Parcel Fabric, then create DataFrame for processing </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pfFields = [f.name for f in arc.ListFields(pfFC)]\n",
    "# print(pfFields)\n",
    "pf_df = features.GeoAccessor.from_featureclass(pfPoints, fields=['Name'])\n",
    "pf_df.Name = pf_df.Name.apply(lambda x: x.strip()) #Remove whitespace from the Name Column\n",
    "print('Parcel Fabric DataFrame created.')\n",
    "# pf_df.shape"
   ]
  },
  {
   "source": [
    "Create DataFrame from Parcel Fabric Polygons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "ppf_df = features.GeoAccessor.from_featureclass(pfFC, fields=['Name'])\n",
    "ppf_df.Name = ppf_df.Name.apply(lambda x: x.strip()) #Remove whitespace from the name column\n",
    "print('Parcel Fabric DataFrame created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create list of columns in DataFrame to check for missing columns. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pfC = pf_df.columns.tolist()\n",
    "# print(pfC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MGO Permits DF Creation and Data Curation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create DataFrame from MGO Table </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mTab = [f for f in cTables if f.name.startswith('MGOPermits') == True][0]\n",
    "# mTable = str(tLoc / mgoT)\n",
    "# print(mTable)\n",
    "df_mgo = features.GeoAccessor.from_table(mTab, skip_nulls= False)\n",
    "print(f'{mTab.name} has been created')\n",
    "# df_mgo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create DataFrame for Permits Description to use for merge with MGO Permits. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to local GIS Folder for Processing\n",
    "appCsv = netDir / 'ParcelWithSuffix' / 'Permits_Desc.csv'\n",
    "locCsv = pProcessing / 'Permits_Desc.csv'\n",
    "# print(locCsv)\n",
    "copy2(appCsv, locCsv)\n",
    "print(f'Copied {locCsv.name} to {locCsv.parent}')\n",
    "\n",
    "time.sleep(2)\n",
    "\n",
    "p_desc = pd.read_csv(locCsv, keep_default_na=False)\n",
    "print('Created Permits Description DataFrame')\n",
    "# p_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns and values have been fixed on the spreadsheet. Does not to be automated due to it never changing.\n",
    "#not for us in production\n",
    "# cFix = p_desc.columns.tolist()\n",
    "# pList = [f.strip() for f in cFix]\n",
    "# print(pList)\n",
    "\n",
    "# cpFix = dict(zip(cFix, pList))\n",
    "# print(cpFix)\n",
    "\n",
    "# p_desc = p_desc.rename(columns=cpFix)\n",
    "# # p_desc\n"
   ]
  },
  {
   "source": [
    "Edit the MGO Dates and format them to match AS400 Data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upper Case all the column names\n",
    "df_mgo = df_mgo.rename(columns=lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdate(field):\n",
    "    from datetime import date\n",
    "    po = date(2021, 1, 1)\n",
    "    if field >= po:\n",
    "        return field\n",
    "    else:\n",
    "        return ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.CODATE = df_mgo.CODATE.apply(fdate)\n",
    "# df_mgo['CODATE'] = pd.to_datetime(df_mgo.CODATE)\n",
    "# df_mgo.drop(columns='CODate1', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mgo.CODATE.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.ISSUEDATE = df_mgo.ISSUEDATE.apply(fdate)\n",
    "# df_mgo['ISSUEDATE'] = pd.to_datetime(df_mgo.CODATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.APPDATE = df_mgo.APPDATE.astype(str)\n",
    "df_mgo.ISSUEDATE = df_mgo.ISSUEDATE.astype(str)\n",
    "df_mgo.CODATE= df_mgo.CODATE.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_mgo['CODATE'] = df_mgo['CODATE'].apply(date)\n",
    "# df_mgo.CODATE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.ISSUEDATE.replace('NaT', '', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date(field):\n",
    "    if field != '':\n",
    "        sp = field.split('-')\n",
    "        return '{}/{}/{}'.format(sp[1], sp[-1], sp[0])\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo.APPDATE = df_mgo.APPDATE.apply(date)\n",
    "df_mgo.CODATE = df_mgo.CODATE.apply(date)\n",
    "df_mgo.ISSUEDATE = df_mgo.ISSUEDATE.apply(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_mgo.drop(columns='OBJECTID', inplace=True)\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# df_mgo['PERMITYEAR'] = df_mgo['PERMITNUMBER'].apply(lambda x: x.split('-')[0])\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo = df_mgo.rename(columns={'APPTYPE' : 'MGOTYPE', 'PERMITSTATUS': 'MGOSTATUS'})\n",
    "# df_mgo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mgo['TYPE'] = df_mgo['PERMITNUMBER'].apply(lambda x: x.split('-')[-1])\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# To find the MGO permits that are not in 2021\n",
    "# df_mgo_drop = df_mgo.loc[df_mgo['PERMITYEAR'] != \"2021\", :]\n",
    "# df_mgo_drop\n",
    "#drop the permits not in 2021\n",
    "df_mgo = df_mgo.loc[df_mgo['PERMITYEAR'] == \"2021\", :].copy(deep=True)\n",
    "# df_mgo\n",
    "# df_mgo_drop.to_csv\n",
    "# df_mgo = df_mgo.loc[df_mgo['PERMITYEAR'] == '2021']\n",
    "# df_mgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mGODict = df_mgo.to_dict('records')\n",
    "# print(mGODict)\n",
    "mCSV = pReview / 'mgoPerms.csv'\n",
    "df_mgo.to_csv(mCSV, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export for Review later to see the outer merge which will return all the values and which tables they are located in.<br>\n",
    "Merge to get status and Description from Permit Description DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export for Review later to see the outer merge which will return all the values and which tables they are located in. \n",
    "df_mgo_desc_o = df_mgo.merge(p_desc[['TYPE', 'DESCRIPTION']], how='outer', on='TYPE', indicator= True) #merge to create description column\n",
    "tEr = pReview / 'Type_Error.csv'\n",
    "df_mgo_desc_o.loc[df_mgo_desc_o._merge == 'left_only', :].to_csv(tEr, index=False) # export csv if any errors\n",
    "df_mgo_desc_o = df_mgo_desc_o.drop(columns='_merge')\n",
    "\n",
    "#merge to get status from Permit Description DataFrame\n",
    "df_mgo_mis = df_mgo_desc_o.merge(p_desc[['MGO_DESC', 'MGO_INIT']], how='outer',\n",
    "left_on='MGOSTATUS', right_on='MGO_DESC', indicator=True) #merge to create status column\n",
    "mgoM = pReview / 'Status_Error.csv'\n",
    "df_mgo_mis.loc[df_mgo_mis._merge == 'left_only'].to_csv(mgoM, index=False)\n",
    "mgo_df = df_mgo_mis.loc[df_mgo_mis._merge == 'both']\n",
    "# df_mgo_mis[df_mgo_mis._merge == 'both']\n",
    "# merMTable = join(locFolder, f'MGOPermitsMerge_{dStr}.csv')\n",
    "#not for production, used to find null values\n",
    "# # df_mgo_mis.loc[df_mgo_mis['_merge'] != 'both', :]\n",
    "# # df_mgo_mis.loc[df_mgo_mis['_merge'] == 'left_only', :]\n",
    "# df_mgo_mis.to_csv(merMTable, index=False)\n",
    "# del df_mgo_desc_o\n",
    "print(\"Exported MGO Permits Merge Table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mgo_mis[df_mgo_mis.PARCELID == '7-00-10302-01-4300-00001']\n",
    "# df_mgo_mis.loc[(df_mgo_mis.PARCELID.duplicated() == True) & (df_mgo_mis._merge == 'both'), :].groupby(by='PARCELID')\n",
    "# print(dList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All_Permits DF Creation and Data Curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Create DataFrame from All_Permits table on Local GDB after exporting from ESRIDB. If arc.env.workspace is changed need to change aPerms to full path.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aTab = [f for f in cTables if f.name.startswith('All_Permits') == True][0]\n",
    "# mTable = str(tLoc / mgoT)\n",
    "# print(mTable)\n",
    "ap_df = features.GeoAccessor.from_table(aTab, skip_nulls= False)\n",
    "print(f'{aTab.name} has been created')\n",
    "# ap_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove whitespace and make it upper\n",
    "ap_df.rename(columns=lambda x: x.strip().upper(), inplace=True)\n",
    "ap_df.drop(columns='OBJECTID', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix a blank entry from PRIDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ap_df['PARCELID'] = ap_df['PARCELID'].apply(lambda x: '7-00-12900-02-5200-00001' if x.strip() == '-00-12900-02-5200-00001' else x)"
   ]
  },
  {
   "source": [
    "Create Date columns for sorting by date later."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def newyear(field):\n",
    "    if int(field) > 80 and int(field) < 100:\n",
    "        return \"19\" + str(field)\n",
    "    elif int(field) >= 0 and int(field) <=9:\n",
    "        return \"20\" + str(field)\n",
    "    elif int(field) > 9 and int(field) < 80:\n",
    "        return \"20\" + str(field)\n",
    "ap_df['YEAR'] = ap_df['YEAR'].apply(newyear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare AS400 DataFrame for merge\n",
    "ap_df['PERMITNUMBER'] = ap_df[['YEAR', 'PERMITNUMBER', 'TYPE']].apply(lambda x: f'{x[0]}-{x[1]}-{x[2]}', axis=1)\n",
    "ap_df = ap_df[ap_df.YEAR != '2021'] #Remove years == 2021\n",
    "ap_df.drop(columns='DESCRIPTION', inplace=True) #will be merged back using an edited description csv\n",
    "ap_df_desc = ap_df.merge(p_desc[['TYPE', 'DESCRIPTION']], on='TYPE', how='outer', indicator= True)\n",
    "ap_df_b = ap_df_desc[ap_df_desc._merge == 'both'].copy(deep=True)\n",
    "ap_df_b.drop(columns=['PERMITDESC', '_merge'], inplace= True)\n",
    "ap_df_b.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Feature Class for Historical Data, contains some geospatial information. Merge parcel fabric parcels with all permits dataframe (ap_df_b). Keep only values that are matching and ones in the all permits dataframe. Fill in shape and name with zeros for export later\n",
    "hPFC = locGDB / f'Historical_Permits_{dStr}'\n",
    "ap_m = pf_df.merge(ap_df_b, how='outer', left_on='Name', right_on='PARCELID', indicator=True)\n",
    "ap_m = ap_m[ap_m._merge != 'left_only']\n",
    "ap_m.drop(columns='_merge', inplace=True)\n",
    "ap_m.SHAPE.fillna(0, inplace=True)\n",
    "ap_m.Name.fillna(0, inplace=True)\n",
    "ap_m.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Export to CSV for review and drop shape column to add as table to local GDB\n",
    "apCsv = pReview / 'Permits_Historical.csv'\n",
    "ap_m.to_csv(apCsv, index=False)\n",
    "tabAP = ap_m.drop(columns='SHAPE').copy(deep=True)\n",
    "\n",
    "#Convert Date Columns to datetime\n",
    "ap_m.APPDATE = pd.to_datetime(ap_m.APPDATE, format='%m/%d/%Y', errors='coerce')\n",
    "ap_m.DATE = pd.to_datetime(ap_m.DATE, format='%m/%d/%Y', errors='coerce')\n",
    "ap_m['PERMITYEAR'] = pd.to_datetime(ap_m.YEAR, format='%Y', errors='coerce')\n",
    "ap_m.CODATE = pd.to_datetime(ap_m.CODATE, format='%m/%d/%Y', errors='coerce')\n",
    "\n",
    "#Create table with historical permits as a table for the values that don't contain geospatial information.\n",
    "tabName = locGDB.parent / f'HPermit_{dStr}'\n",
    "tabAP.spatial.to_table(str(tabName))\n",
    "\n",
    "#fill in the zeros in SHAPE column with fake spatialReference\n",
    "fsr = {'x': None, 'y': None, 'spatialReference':{'wkid':26957}}\n",
    "ap_m.SHAPE = ap_m.SHAPE.apply(lambda x: fsr if x == 0 else x)\n",
    "\n",
    "#export to feature class\n",
    "ap_m.spatial.to_featureclass(f'{hPFC}', sanitize_columns=False)\n",
    "\n",
    "print(f'Exported Historical Permits to {hPFC.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export FC from local GDB to ESRIDB\n",
    "dbHPerms = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.InspectionandEnforcement\" / \"PROD.GISADMIN.HistoricalPermit\"\n",
    "# arc.FeatureClassToFeatureClass_conversion(str(hPFC), f'{dbHPerms.parent}', f'{dbHPerms.name}')\n",
    "arc.TruncateTable_management(str(dbHPerms))\n",
    "arc.Append_management(str(hPFC), str(dbHPerms), schema_type='NO_TEST')\n",
    "print(f'Appended {hPFC.name} to {dbHPerms.name}')\n",
    "\n",
    "#Export table from local GDB to ESRIDB\n",
    "dbHtab = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.HistPermit\"\n",
    "# arc.TableToTable_conversion(str(tabName), f'{dbHtab.parent}', f'{dbHtab.name}')\n",
    "arc.TruncateTable_management(str(dbHtab))\n",
    "arc.Append_management(str(tabName), str(dbHtab), schema_type='NO_TEST')\n",
    "print(f'Appended {tabName.name} to {dbHtab.name}')"
   ]
  },
  {
   "source": [
    "DO NOT USE UNLESS THE PREVIOUS CELL ISN'T EXPORTING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Do not use unless ArcGIS module isn't exporting correctly.\n",
    "\n",
    "# ap_m.loc[ap_m.SHAPE.isnull(), :]\n",
    "ap_m.spatial.set_geometry('SHAPE', sr=26957)\n",
    "\n",
    "lal = gisPath / 'HPermits.csv'\n",
    "ap_m.to_csv(lal, index=False)\n",
    "\n",
    "import geopandas as gpd\n",
    "papa = pd.read_csv(lal)\n",
    "p2p = gpd.GeoDataFrame(papa, geometry='SHAPE', crs=26957)\n",
    "\n",
    "pmdf = features.GeoAccessor.from_df(ap_m, sr=26957, geometry_column='SHAPE')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Duplicate Parcel IDs\n",
    "dup_ap = ap_df_b.sort_values(by='YEAR')\n",
    "dup_ap = ap_df_b.drop_duplicates(subset=['YEAR', 'PARCELID', 'TYPE'], keep='last', ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop RJ and HD status from the permits\n",
    "dup_ap = dup_ap.loc[(dup_ap.STATUS != 'RJ') | (dup_ap.STATUS != 'HD')].copy(deep=True)\n",
    "# dup_ap.STATUS.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_ap = dup_ap.merge(p_desc[['AS_DESC', 'AS_INIT']], how='left', left_on='STATUS', right_on='AS_INIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aPerm = pReview / 'AS_Perms.csv'\n",
    "dup_ap.to_csv(aPerm, index=False)"
   ]
  },
  {
   "source": [
    "## Merge MGO Permits and AS400 Permits into one table and get geospatial information from Parcel Fabric Points."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop and rename columns from AS400 and MGO DataFrames\n",
    "mgo_df.drop(columns='_merge', inplace=True)\n",
    "dup_ap.drop(columns='AS_INIT', inplace=True)\n",
    "dup_ap.rename(columns={'AS_DESC' : 'STATDESC'}, inplace=True)\n",
    "\n",
    "#Remove Parcel #s that are in AS400 Data (dup_ap)\n",
    "pmf = mgo_df[~mgo_df.PARCELID.isin(dup_ap.PARCELID)].copy(deep=True)\n",
    "pmf.rename(columns={'AS_DESC' : 'STATDESC'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the pmf with dup_ap to get a list of all permits from MGO and AS400\n",
    "perm_df = pmf.merge(dup_ap, on='PARCELID', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat the columns into one.\n",
    "perm_df['TYPE'] = perm_df.TYPE_x.str.cat(perm_df.TYPE_y, na_rep='')\n",
    "perm_df['PERMITNUMBER'] = perm_df.PERMITNUMBER_x.str.cat(perm_df.PERMITNUMBER_y, na_rep='')\n",
    "perm_df['DESCRIPTION'] = perm_df.DESCRIPTION_x.str.cat(perm_df.DESCRIPTION_y, na_rep='')\n",
    "perm_df['APPDATE'] = perm_df.APPDATE_x.str.cat(perm_df.APPDATE_y, na_rep='')\n",
    "perm_df['CODATE'] = perm_df.CODATE_x.str.cat(perm_df.CODATE_y, na_rep='')\n",
    "perm_df['ISSUED'] = perm_df.ISSUEDATE.str.cat(perm_df.DATE, na_rep='')\n",
    "perm_df['STATUSDESC'] = perm_df.STATDESC.str.cat(perm_df.MGO_DESC, na_rep='')\n",
    "perm_df['PERMSTATUS'] = perm_df.STATUS.str.cat(perm_df.MGO_INIT, na_rep='')\n",
    "perm_df['PERMYEAR'] = perm_df.YEAR.str.cat(perm_df.PERMITYEAR, na_rep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Columns to keep for the Feature Class on ESRIDB and the REST Server\n",
    "pmCols = ['PARCELID', 'TYPE', 'PERMSTATUS', 'ISSUED', 'PERMYEAR', 'PERMITNUMBER', 'DESCRIPTION', 'SHAPE', 'CODATE', 'APPDATE']\n",
    "pmFields = [f.name for f in arc.ListFields('PROD.GISADMIN.Permits')][1:]\n",
    "pmFields.remove('PermitNumb')\n",
    "kpmCols = dict(zip(pmCols, pmFields))\n",
    "kpmCols['STATUSDESC'] = 'StatusDesc'\n",
    "pmCols.append('STATUSDESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = perm_df.drop(columns=[f for f in perm_df.columns if f not in pmCols])\n",
    "#Rename Columns\n",
    "pm_df.rename(columns=kpmCols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the date columns to datetime\n",
    "pm_df.AppDate = pd.to_datetime(pm_df.AppDate, format='%m/%d/%Y', errors='coerce')\n",
    "pm_df.IssueDate = pd.to_datetime(pm_df.IssueDate, format='%m/%d/%Y', errors='coerce')\n",
    "pm_df['Year'] = pd.to_datetime(pm_df.PermitYear, format='%Y', errors='coerce')\n",
    "pm_df.CODate = pd.to_datetime(pm_df.CODate, format='%m/%d/%Y', errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pm_df.columns)\n",
    "print(pf_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Parcel Fabric Points with Permit Data\n",
    "pfm_df = pf_df.merge(pm_df, on='Name', how='outer', indicator=True)\n",
    "# pfm_df = pm_df.merge(pf_df, on='Name', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s_pm is all the permits with suffix and t_pm is parcels that aren't in Parcel Fabric\n",
    "sOut = pReview / 'Permits_Suffix.csv'\n",
    "tOut = pReview / 'Permits_Parcels_NM.csv' #NM = No Match\n",
    "s_pm = pfm_df[(pfm_df._merge != 'left_only') & (~pfm_df.Name.str.contains('-00001', na=False))]\n",
    "t_pm = pfm_df[(pfm_df._merge != 'left_only') & (pfm_df.Name.str.contains('-00001', na=False))]\n",
    "s_pm.drop(columns='_merge', inplace=True)\n",
    "s_pm.to_csv(sOut, index=False)\n",
    "t_pm.to_csv(tOut, index=False)\n",
    "\n",
    "print(f'Exported {sOut.name} and {tOut.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join suffix permits with parcel with suffix. Merge back into Main Permits afterwards. Need to run this after running the parcel with suffix script.\n",
    "sParcels = f'{locGDB / f\"ParcelWithSuffix_{dStr}\"}'\n",
    "sPar = features.GeoAccessor.from_featureclass(sParcels, fields=['Name'])\n",
    "\n",
    "#Merge suffix parcels and suffix permits to create a new DF\n",
    "sufMerge = sPar.merge(s_pm, on='Name', how='outer', indicator=True)\n",
    "sufPerm = sufMerge[sufMerge._merge == 'both'].copy(deep=True)"
   ]
  },
  {
   "source": [
    "Append Suffix Permits and Parcel Permits"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the DataFrames for concat\n",
    "sufPerm.drop(columns='SHAPE_y', inplace=True)\n",
    "sufPerm.rename(columns={'SHAPE_x':'SHAPE'}, inplace=True)\n",
    "#Concat suffix and parcels\n",
    "perm_all = pd.concat([sufPerm, t_pm], axis=0)\n",
    "print(f'Concatenated DataFrames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the PermitYear a year column and Year a string column\n",
    "perm_all['PermitYear'] = pd.to_datetime(perm_all.Year, format='%Y', errors='coerce')\n",
    "perm_all.Year = perm_all.Year.astype(str).apply(lambda x: x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to All Permit before dropping the duplicates\n",
    "permEx = perm_all[perm_all._merge != 'right_only'].copy(deep=True)\n",
    "permEx.drop(columns='_merge', inplace=True)\n",
    "permEx.SHAPE.fillna(0, inplace=True)\n",
    "permEx.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Convert Date Columns to datetime\n",
    "permEx.AppDate = pd.to_datetime(permEx.AppDate, format='%m/%d/%Y', errors='coerce')\n",
    "# perm_df['PermitYear'] = pd.to_datetime(perm_df.Year, format='%Y', errors='coerce')\n",
    "permEx.CODate = pd.to_datetime(permEx.CODate, format='%m/%d/%Y', errors='coerce')\n",
    "# perm_df.Year = perm_df.Year.astype(str)\n",
    "\n",
    "#Export to CSV for review and drop SHAPE column to add to table\n",
    "permCSV = pReview / 'AllPermits.csv'\n",
    "permEx.to_csv(permCSV, index=False)\n",
    "tabPerm = permEx.drop(columns='SHAPE').copy(deep=True)\n",
    "\n",
    "#Create table with All Permits as a table for the values that don't contain geospatial information\n",
    "tabPermN = outGDB / f'AllPTable_{dStr}'\n",
    "# tabPerm.spatial.to_table(str(tabPermN))\n",
    "print(f'Exported {tabPermN.name} to {outGDB.name}')\n",
    "\n",
    "#Fill in the zeros in the SHAPE column with fake spatialReference\n",
    "permEx.SHAPE = permEx.SHAPE.apply(lambda x: fsr if x == 0 else x)\n",
    "\n",
    "#Export the dataframe to local GDB\n",
    "outFC = locGDB / f'AllPermits_{dStr}'\n",
    "perm_df.spatial.to_featureclass(f'{outFC}', sanitize_columns=False)\n",
    "print(f'Exported {outFC.name} to {locGDB.name}')\n",
    "\n",
    "#Export from local GDB to ESRIDB\n",
    "dbPerms = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.InspectionandEnforcement\" / \"PROD.GISADMIN.AllPermit\"\n",
    "arc.TruncateTable_management(str(dbPerms))\n",
    "arc.Append_management(str(outFC), str(dbPerms), schema_type='NO_TEST')\n",
    "print(f'Appending {outFC.name} to {dbPerms.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Permits to GDB and append to Permits on the DB.\n",
    "perm_db = locGDB / f'Permits_DB_{dStr}'\n",
    "dup_p = perm_all[perm_all._merge == 'both'].copy(deep=True)\n",
    "demoDrop = dup_p.loc[(dup_p.StructureType == \"DEMO\") | (dup_p.StructureType == \"DEMS\") | (dup_p.StructureType == \"MHDM\")]\n",
    "dup_p.drop(demoDrop.index, axis=0, inplace=True)\n",
    "dup_p.reset_index(drop=True, inplace=True)\n",
    "dup_p.sort_values(by='AppDate', inplace=True)\n",
    "dup_p.drop_duplicates(subset=['Name'], keep='last', ignore_index=True, inplace=True)\n",
    "dup_p.drop(columns='_merge', inplace=True)\n",
    "\n",
    "# #export to Local GBD\n",
    "dup_p.spatial.to_featureclass(perm_db, sanitize_columns=False)\n",
    "print(f'Exported {perm_db.name} to {locGDB.name}')\n",
    "\n",
    "#export to CurrentPermits on ESRIDB\n",
    "dbCPerms = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.InspectionandEnforcement\" / \"PROD.GISADMIN.CurrentPermit\"\n",
    "arc.TruncateTable_management(str(dbCPerms))\n",
    "arc.Append_management(str(perm_db), str(dbCPerms), schema_type='NO_TEST')\n",
    "print(f'Appended {perm_db.name} to {dbCPerms.name}')"
   ]
  },
  {
   "source": [
    "### Create DEMO Permits Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Create AS400 Demo Permits DataFrame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change Date columns in pm_df\n",
    "pm_df['PermitYear'] = pd.to_datetime(pm_df.Year, format='%Y', errors='coerce')\n",
    "pm_df.Year = pm_df.Year.astype(str).apply(lambda x: x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Export Demo Permits for Review\n",
    "dCsv = pReview / 'DemoPermits.csv'\n",
    "df_demo = pm_df.query('StructureType == \"DEMO\" | StructureType == \"DEMS\" | StructureType == \"MHDM\"')\n",
    "# df_demo.reset_index(drop=True, inplace=True)\n",
    "df_demo.to_csv(dCsv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Parcel Fabric Points and df_demo Data\n",
    "demo_fc = pf_df.merge(df_demo, on='Name', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export Parcel Names that don't match\n",
    "dLo = pReview / 'Missing_Demo_P.csv'\n",
    "demo_fc[demo_fc._merge == 'right_only'].to_csv(dLo, index=False)"
   ]
  },
  {
   "source": [
    "#Create dummy polygon spatial Reference to replace the null values in SHAPE. This will allow you to export to a feature class\n",
    "locDemo = demo_fc[demo_fc._merge != 'left_only'].copy(deep=True)\n",
    "locDemo.drop(columns='_merge', inplace=True)\n",
    "locDemo.SHAPE.fillna(0, inplace=True)\n",
    "locDemo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Export to CSV for review and drop shape column to add as a table to local gdb\n",
    "dmCSV = pReview / 'All_Demo_Permits.csv'\n",
    "locDemo.to_csv(dmCSV, index=False)\n",
    "tabDemo = locDemo.drop(columns='SHAPE').copy(deep=True)\n",
    "\n",
    "#Export table to local gdb for values that don't contain geospatial information\n",
    "tabDN = outGDB / f'AllDPermits_{dStr}'\n",
    "locDemo.spatial.to_table(str(tabDN))\n",
    "print(f'Exported {tabDN.name} to {outGDB.name}')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the Demo Permits DataFrame as a feature class to the local GDB. Not all values contain geospatial information.\n",
    "# pfsr = {'rings': [[[0,0]]], 'spatialReference': {'wkid': 26957, 'latestWkid': 26957}} #Use if they are polygons\n",
    "fsr = {'x': None, 'y': None, 'spatialReference':{'wkid':26957}}\n",
    "#Fill in the zeros in the SHAPE column with fake spatialReference\n",
    "locDemo.SHAPE = locDemo.SHAPE.apply(lambda x: fsr if x == 0 else x)\n",
    "\n",
    "#Export the dataframe to local GDB\n",
    "outDemo = locGDB / f'AllDemoPermits_{dStr}'\n",
    "locDemo.spatial.to_featureclass(f'{outDemo}', sanitize_columns=False)\n",
    "print(f'Exported {outDemo.name} to {locGDB.name}')\n",
    "\n",
    "#Export from local GDB to ESRIDB\n",
    "dbDPerms = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.InspectionandEnforcement\" / \"PROD.GISADMIN.AllDemoPermit\"\n",
    "arc.TruncateTable_management(str(dbDPerms))\n",
    "arc.Append_management(str(outDemo), str(dbDPerms), schema_type='NO_TEST')\n",
    "print(f'Appending {outDemo.name} to {dbDPerms.name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suffix Parcels to be merged with suffix layer\n",
    "sDem = pReview / 'Suffix_Parcels.csv'\n",
    "demo_fc[(demo_fc._merge != 'left_only') & (~demo_fc.Name.str.contains('-00001', na=False))].to_csv(sDem, index=False)\n",
    "\n",
    "#Merge Suffix with demo_fc data\n",
    "sdem_df = demo_fc[(demo_fc._merge != 'left_only') & (~demo_fc.Name.str.contains('-00001', na=False))].copy(deep=True)\n",
    "sdem_df.drop(columns=['SHAPE','_merge'], inplace=True)\n",
    "sdem_df.reset_index(drop=True, inplace=True)\n",
    "sMD = sPar.merge(sdem_df, how='outer', left_on='Name', right_on='Name', indicator=True)\n",
    "\n",
    "#Export the missing parcels to review\n",
    "sufOut = pReview / 'Missing_Suffix.csv'\n",
    "sMD[sMD._merge == 'right_only'].to_csv(sufOut, index=False)\n",
    "\n",
    "#Concat the matching into demo_fc\n",
    "sMD = sMD[sMD._merge == 'both'].copy(deep=True)\n",
    "dDem = demo_fc[(demo_fc.Name.str.contains('-00001', na=False)) & (demo_fc._merge != 'left_only')].copy(deep=True)\n",
    "dDem.drop(columns='_merge', inplace=True)\n",
    "sMD.drop(columns='_merge', inplace=True)\n",
    "fDemo = [dDem, sMD]\n",
    "cDem = pd.concat(fDemo)\n",
    "cDem.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the Demo Permits File to Local GDB\n",
    "demoPerm = locGDB / f'DemoPermits_{dStr}'\n",
    "cDem.sort_values(by='AppDate', inplace=True)\n",
    "cDem.drop_duplicates(subset=['Name'], keep='last', ignore_index=True, inplace=True)\n",
    "cDem.SHAPE.fillna(0,inplace=True)\n",
    "cDem.IssueDate.fillna('', inplace=True)\n",
    "cDem.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#Fill in the zeros in the SHAPE column with fake spatialReference\n",
    "cDem.SHAPE = cDem.SHAPE.apply(lambda x: fsr if x == 0 else x)\n",
    "\n",
    "#Remove columns that won't have any values.\n",
    "cDem.drop(columns='CODate', inplace=True)\n",
    "\n",
    "#Export to Local GBD\n",
    "cDem.spatial.to_featureclass(str(demoPerm), sanitize_columns=False)\n",
    "print(f'Exported {demoPerm.name} to {locGDB.name}')\n",
    "\n",
    "#export to CurrentPermits on ESRIDB\n",
    "dbDPerms = netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.InspectionandEnforcement\" / \"PROD.GISADMIN.CurrentDemoPermit\"\n",
    "arc.TruncateTable_management(str(dbDPerms))\n",
    "arc.Append_management(str(demoPerm), str(dbDPerms), schema_type='NO_TEST')\n",
    "print(f'Appended {demoPerm.name} to {dbDPerms.name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0be568a45c5be0d4caf76dff1c2f5b3b93950ef2ed99d2899e809934a8a7c9d46",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}