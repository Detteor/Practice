{
 "cells": [
  {
   "source": [
    "# Create Feature Class with the Sheriff Sales values, Add to Employee Map, Add to Sheriff Map, and send an email to Kathy Skinner telling her of the mismatched values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    " <b> TO DO: </b>\n",
    " <ul> \n",
    "        <li><del>Edit Address DataFrame for merge in case the ParcelID merge doesn't have a match.</li>\n",
    "        <li>Use spatial join when the parcels do not match. Test spatial join with python api. If not use geopandas spatial join.</li>\n",
    "        <li>Do not use arc.env.workspace Environment Variables.</li>\n",
    "        <li>Add to Daily Script.</li>  \n",
    "        <li>Automate sending an email to Kathy Skinner in the Sheriff's Office when they don't match</li>\n",
    "</ul> "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modules to import\n",
    "import arcpy as arc\n",
    "import pandas as pd\n",
    "from arcgis import GIS, features\n",
    "import csv\n",
    "import pendulum\n",
    "from pathlib import Path\n",
    "from shutil import copy2 as cp\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "### FOR USE WITH NOTEBOOK ONLY"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 40"
   ]
  },
  {
   "source": [
    "## Create Folders in the Users Directory since it should have r/w permissions for the user."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'"
   ]
  },
  {
   "source": [
    "## Create File GeoDatabase and Feature Datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Create Folders for Sheriff Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Variables\n",
    "today = pendulum.today()\n",
    "fMon_Str = today.first_of('month').strftime('%m/%d/%Y')\n",
    "# fMon.strftime('%m/%d/%Y')\n",
    "tStr = today.strftime('%m/%d/%Y')\n",
    "# print(tStr)\n",
    "now = pendulum.now()\n",
    "mDate = now.strftime('%m%Y')\n",
    "dDate =  now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "# revPath = uPath / 'GIS' / 'Review'\n",
    "# gisPath = uPath / 'GIS' / 'Processing'\n",
    "csvDir = Path(r'\\\\esripub2\\SalesWebExport')\n",
    "saleCsv = csvDir / 'SalesWebExport.csv'\n",
    "# sPath = revPath / 'Sheriff' / f'{dDate}'\n",
    "# sPath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "#Create Folders for ParcelWithSuffix Data\n",
    "sFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "sProcessing = sFolder / 'Sheriff' / f'{dDate}'\n",
    "if sProcessing.exists() == True:\n",
    "    print(f'{sProcessing} already exist.')\n",
    "else:\n",
    "    sProcessing.mkdir(parents=True)\n",
    "    print(f'Created {sProcessing}.')\n",
    "\n",
    "sFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "sReview = sFR / 'Sheriff' / f'{dDate}'\n",
    "if sReview.exists() == True:\n",
    "    print(f'{sReview} already exist')\n",
    "else:\n",
    "    sReview.mkdir(parents=True)\n",
    "    print(f'Created {sReview}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "sr = arc.Describe(f'{iE}').spatialReference\n",
    "outGDB = gisPath / sFolder / f'Data_{mDate}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dDate}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{sFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "source": [
    "## Curate the CSV file to be used with ArcGIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy the csv file from network drive to local drive for processing and create DataFrame \n",
    "dropFields = ['StreetNumber', 'StreetPrefix', 'Address1', 'StreetType']\n",
    "try:\n",
    "    cp(saleCsv, sProcessing)\n",
    "    print(f'Copied {saleCsv.name} to {sProcessing}')\n",
    "except:\n",
    "    print(f'Could not copy {saleCsv.name}')\n",
    "# in the script check to make sure that the user has access to the server and can copy the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can use Pandas, CSV module, or generators/iterators\n",
    "# lines = (line for line in netCsv.open())\n",
    "# list_line = (s.rstrip().split(',') for s in lines)\n",
    "# cols = next(list_line)\n",
    "# print(list(data))\n",
    "# sher_dict = (dict(zip(cols,data)) for data in list_line)\n",
    "# d = [k for k,v in sher_dict]\n",
    "# print(d)\n",
    "# jAmount = (p['\"ApproxJudgment\"'] for p in sher_dict)\n",
    "# l = [k for k,v in sher_dict]\n",
    "# print(f'Blank {l}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv(saleCsv, na_filter= False)\n",
    "print(f\"Created DF from {saleCsv.name}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage Data by changing the Prefix to match the Address Format in the Address Feature Class\n",
    "columns = sales_df.columns.to_list()\n",
    "# print(columns)\n",
    "pre = sales_df.StreetPrefix.unique()\n",
    "# print(pre)\n",
    "for p in pre:\n",
    "    if p.upper() == 'NORTH':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'N').strip())\n",
    "    elif p.upper() == 'EAST':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'E').strip())\n",
    "    elif p.upper() == 'WEST':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'W').strip())\n",
    "    elif p.upper() == 'SOUTH':\n",
    "        sales_df['StreetPrefix'] = sales_df.StreetPrefix.apply(lambda x: x.replace(p, 'S').strip())\n",
    "    else:\n",
    "        sales_df.StreetPrefix\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manage Data by changing the Prefix to match the Address Format in the Address Feature Class\n",
    "sType = sales_df.StreetType.unique()\n",
    "# print(sType)\n",
    "for d in sType:\n",
    "    if d.upper() == 'ROAD':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'RD').strip())\n",
    "    elif d.upper() == 'DRIVE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "    elif d.upper() == 'BOULEVARD':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "    elif d.upper() == 'LANE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "    elif d.upper() == 'COURT':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "    elif d.upper() == 'CIRCLE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "    elif d.upper() == 'STREET':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "    elif d.upper() == 'HIGHWAY':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "    elif d.upper() == 'AVENUE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "    elif d.upper() == 'PLACE':\n",
    "        sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "    else:\n",
    "        sales_df.StreetType.str.strip()\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Address from other fields and remove white space\n",
    "# for d in pre:\n",
    "#     if d == ' ':\n",
    "#         sales_df['Address'] = sales_df.StreetNumber + ' ' + sales_df.Address1 + ' ' + sales_df.StreetType      \n",
    "#     else:\n",
    "#         sales_df['Address'] = sales_df.StreetNumber + ' ' + sales_df.StreetPrefix + ' ' + sales_df.Address1 + \\\n",
    "#         ' ' + sales_df.StreetType\n",
    "# sourceCol = sales_df.columns.get_loc('StreetNumber')\n",
    "sales_df['Add'] = sales_df[['StreetNumber', 'StreetPrefix', 'Address1', 'StreetType']].values.tolist()\n",
    "sales_df['Addr'] = sales_df['Add'].apply(' '.join)\n",
    "sales_df['Address'] = sales_df['Addr'].apply(lambda x: ' '.join(x.split()))\n",
    "# sales_df.iloc[sales_df[''].apply\n",
    "sales_df.drop(columns=['Add', 'Addr'], inplace=True)\n",
    "# sales_df.assign(Address = )\n",
    "# sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If Address2 field is notnull or na then move value into Address field. Do this after address matching when using df_add to merge.\n",
    "if len(sales_df.loc[sales_df.Address2 != '']) > 0:\n",
    "    sales_df.Address = np.where(sales_df.Address2 != '', sales_df.Address2, sales_df.Address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check dates and remove dates that are before current date\n",
    "sales_df['PropStatus'] = sales_df.PropertyStatusDate\n",
    "sales_df.PropertyStatusDate = pd.to_datetime(sales_df.PropertyStatusDate, format='%m/%d/%Y', errors='ignore', exact=True)\n",
    "\n",
    "#Change fMon_Str to tStr when removing by current date\n",
    "sales_df = sales_df.loc[sales_df.PropertyStatusDate >= fMon_Str, :]\n",
    "sales_df.drop(columns='PropertyStatusDate', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter out certain action types and property statuses\n",
    "sales_df = sales_df.loc[sales_df.ActionTypeDescription != 'Personal Property']\n",
    "sales_df = sales_df.loc[sales_df.PropertyStatusDescription != 'Stayed']\n",
    "\n",
    "#need to create a note section if the parcel number contains a &. Call count to see if there are any values that meet this requirement. Need to remove the & from the column\n",
    "sValue = sales_df.ParcelNumber.str.count('\\&').tolist()\n",
    "for s in sValue:\n",
    "    if s == 1:\n",
    "        sales_df['Notes'] = sales_df.loc[sales_df.ParcelNumber.str.contains('\\&')].values + ': This parcel is also included in the listed sale'\n",
    "    else:\n",
    "        print('There are no additional parcels.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Holds all the values with a Parcel Number\n",
    "sales_df.ParcelNumber = sales_df.ParcelNumber.replace('', np.nan)\n",
    "sales_df\n",
    "#Handle missing Parcel Number values\n",
    "nan_df = sales_df.loc[sales_df.ParcelNumber.isna(), :]\n",
    "print(len(nan_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variable to hold the number of values in each df to not forget any\n",
    "sValues = sales_df.ParcelNumber.count().tolist()\n",
    "print(sValues)\n",
    "nValues = nan_df.SheriffNumber.count().tolist()\n",
    "print(nValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create CSV for nan values\n",
    "nCSV = sReview / 'None_Values.csv'\n",
    "nan_df.to_csv(nCSV, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales_df_test['Address'] = sales_df_test['Address'].apply(lambda x: x.strip())\n",
    "# lol = sales_df.Addr.unique().tolist()\n",
    "# print(lol)\n",
    "# for x in lol:\n",
    "#     print(x + ' ' + str(len(x)))\n",
    "# for x in lol:\n",
    "#     a = ' '.join(x.split())\n",
    "#     print(x + ' ' + str(len(a)))"
   ]
  },
  {
   "source": [
    "Check to see if there are any parcel #'s with an & in the field. If so writes the value to a new column called \"Notes\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = '9-00-1111 & aksdasd0asdasd'\n",
    "# print(x.split('&')[-1])\n",
    "# ePar = sales_df.ParcelNumber.unique()\n",
    "# for p in ePar:\n",
    "#     if len(p.split('&')[-1]) > 1:\n",
    "#         print(p)\n",
    "#     else:\n",
    "#         print('Nope')\n",
    "# idx = sales_df.ParcelNumber.apply(lambda x: x.split('&')[-1])\n",
    "# print(idx)\n",
    "# sales_df['Notes'] = ''\n",
    "# sales_df.loc[idx, 'Notes'] = str(sales_df.ParcelNumber.str.contains('&', na=False)).split('&')[-1] + ': This parcel is also included in the listed sale'\n",
    "# sales_df\n",
    "# sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Addressing and Parcels using spatial DataFrame from arcgis to merge\n",
    "#Need to change these from the hard values when running the script\n",
    "# locGDB = gisPath / 'Data_042021.gdb' / f'Daily_{dDate}'\n",
    "gdbPar = locGDB / f'ParcelFabric_Parcels_{dDate}'\n",
    "gdbAdd = locGDB / f'Addressing_{dDate}'\n",
    "\n",
    "df_add = features.GeoAccessor.from_featureclass(gdbAdd)\n",
    "# exp = \"TYPE = 7 AND Historical = 0\"\n",
    "df_par = features.GeoAccessor.from_featureclass(gdbPar, fields=['Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame from merge with parcels and sales\n",
    "merge_par = sales_df.merge(df_par, how='outer', right_on='Name', left_on='ParcelNumber', indicator=True)\n",
    "# merge_par.columns\n",
    "iList = merge_par._merge.unique().tolist()\n",
    "# print(iList)\n",
    "#Create csvs from merge for review later\n",
    "for i in iList:\n",
    "    if i == 'right_only':\n",
    "        a = sReview / 'Par_Sales_Right.csv'\n",
    "        merge_par.loc[merge_par._merge == 'right_only', :].to_csv(a, index=False)\n",
    "        print(f'Created {a.stem}')\n",
    "    elif i == 'left_only':\n",
    "        a = sReview / 'Par_Sales_Left.csv'\n",
    "        merge_par.loc[merge_par._merge == 'left_only', :].to_csv(a, index=False)\n",
    "        print(f'Created {a.stem}')\n",
    "    elif i == 'both':\n",
    "        a = sReview / 'Par_Sales_Both.csv'\n",
    "        merge_par.loc[merge_par._merge == 'both', :].to_csv(a, index=False)\n",
    "        print(f'Created {a.stem}')\n",
    "# rCSV = sPath / 'Par_Sales_Right.csv'\n",
    "# lCSV = sPath / 'Par_Sales_Left.csv'\n",
    "# bCsv = sPath / 'Par_Sales_Both.csv'\n",
    "# merge_par.loc[merge_par._merge == 'right_only', :].to_csv(index=False)\n",
    "\n",
    "#Number of values from merge \n",
    "pValues = len(merge_par.loc[merge_par._merge == 'both', :])\n",
    "# print(pValues)\n",
    "\n",
    "if pValues == sValues:\n",
    "    merge_par_b = merge_par.loc[merge_par._merge == 'both', :].copy(deep=True)\n",
    "    merge_par_b.drop(columns=['_merge', 'StreetNumber', 'StreetPrefix', 'StreetType', 'StreetSuffix', 'Address1', 'Address2', 'Address3', 'Name'], inplace=True)\n",
    "    print('All Features are matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to get the fields to match. Most likely will create new feature classes to use.\n",
    " \n",
    "# netDir = Path('G:\\\\MasterGISFiles\\\\Ben')\n",
    "# netDB = netDir / 'GISPro' / 'SDE Connections'\n",
    "# iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "arc.env.workspace = f'{iE.parent}'\n",
    "# print(locGDB)\n",
    "fcList = [f for f in arc.ListFeatureClasses(feature_dataset='PROD.GISADMIN.SheriffSales')]\n",
    "# print(fcList)\n",
    "fList = [f.name for f in arc.ListFields(f'{iE.parent / \"PROD.GISADMIN.SheriffSales\" / \"PROD.GISADMIN.BothSales\"}')]\n",
    "# for x in fcList:\n",
    "#     a = arc.ListFields(f'{iE.parent / 'PROD.GISADMIN.SheriffSales' / x}')\n",
    "#     for l in a:\n",
    "#         fList.\n",
    "#         fList.append(arc.ListFields(x))\n",
    "# print(fList)\n",
    "\n",
    "sCols = sales_df.columns.tolist()\n",
    "cols = dict(zip(sCols, fList))\n",
    "# print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_par_b.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_par.drop(columns=['StreetNumber', 'StreetPrefix', 'StreetType', 'StreetSuffix', 'Address1', 'Address2', 'Address3', 'Name'], inplace=True)\n",
    "# merge_par = merge_par[['SheriffNumber'] + [col for col in merge_par.columns if col !='SheriffNumber']]\n",
    "# print(list(merge_par_b))\n",
    "# cols = list(merge_par_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_par_b.SheriffNumber = merge_par_b.SheriffNumber.astype(np.int32)\n",
    "merge_par_b.ActionTypeId = merge_par_b.ActionTypeId.astype(np.int32)"
   ]
  },
  {
   "source": [
    "If values from merge is zero then use df_add and create spatial join <br>\n",
    "If values from sales_df = values from df_par then create Sheriff feature class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shFC = locGDB / f'Sheriff_Final_{dDate}'\n",
    "if pValues == sValues:\n",
    "    merge_par_b.spatial.to_featureclass(shFC, sanitize_columns=False)\n",
    "    print(f\"Created {shFC.stem} at {locGDB.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy to ESRIDB from local GDB\n",
    "dbSher = f'{netDB / \"PROD-GISADMIN.sde\" / \"PROD.GISADMIN.SheriffSales\" / \"PROD.GISADMIN.SheriffSale\"}'\n",
    "arc.TruncateTable_management(dbSher)\n",
    "arc.Append_management(str(shFC), dbSher, schema_type='NO_TEST')\n",
    "print('Updated Sheriff Sale Feature Class')"
   ]
  },
  {
   "source": [
    "### Edit the Address DataFrame (df_add) to replace the null values in FullAddr column. Only use if the Parcel Fabric Layer and the sales speadsheet don't fully match."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add['STS'].replace('None', \" \")\n",
    "# df_add.loc[df_add.STS == None, :]\n",
    "# df_add.isnull().sum()\n",
    "df_add['STS'].fillna(np.nan, inplace= True)\n",
    "df_add['STS'].replace(' ', '', inplace=True)\n",
    "df_add['STS'].replace(np.nan, '', inplace=True)\n",
    "# df_add['STS'].unique()\n",
    "# df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aType = df_add.STS.unique()\n",
    "# print(sType)\n",
    "for d in aType:\n",
    "    if d.upper() == 'ROAD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'RD').strip())\n",
    "    elif d.upper() == 'DRIVE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "    elif d.upper() == 'BOULEVARD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "    elif d.upper() == 'LANE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "    elif d.upper() == 'COURT':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "    elif d.upper() == 'CIRCLE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "    elif d.upper() == 'STREET':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "    elif d.upper() == 'HIGHWAY':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "    elif d.upper() == 'AVENUE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "    elif d.upper() == 'PLACE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "    else:\n",
    "        df_add.STS.str.strip()\n",
    "print(df_add.STS.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spatially Join Addressing and Parcels. Need to project Addressing to the same SR\n",
    "# df_add = df_add.spatial.project()\n",
    "# df_join = df_par.spatial.join(df_add, left_tag='p_', right_tag='a_')\n",
    "# df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.fillna(np.nan, inplace= True)\n",
    "# df_add.replace(' ', '', inplace=True)\n",
    "df_add.replace(np.nan, '', inplace=True)\n",
    "# print(df_add.columns)\n",
    "print(df_add.STS.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.SAN = df_add.SAN.astype('str',errors='ignore')\n",
    "df_add.SAN = df_add.SAN.apply(lambda x: x.split('.')[0])\n",
    "df_add['FAddress'] = df_add[['SAN', 'PRD', 'STN', 'STS', 'POD']].values.tolist()\n",
    "# df_add.FAddress.values\n",
    "df_add['FooAddress'] = df_add['FAddress'].apply(' '.join)\n",
    "df_add['FoAddress'] = df_add['FooAddress'].apply(lambda x: ' '.join(x.split()))\n",
    "# df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.FAddress = df_add.FoAddress\n",
    "kColumns = ['FAddress', 'SHAPE']\n",
    "# print([x for x in kColumns if x not in kColumns])\n",
    "df_add.drop(columns=[x for x in df_add if x not in kColumns], inplace=True)\n",
    "df_add"
   ]
  },
  {
   "source": [
    "Create DataFrame from merge with address and sales, create csvs from merge for review later, and get number of values after the merge."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame from merge with address and sales\n",
    "sales_df['lAdd'] = sales_df.Address.str.upper()\n",
    "# del merge_add\n",
    "merge_add = df_add.merge(sales_df, how='outer', left_on='FAddress', right_on='lAdd', indicator=True, )\n",
    "# merge_add\n",
    "\n",
    "#create csvs from merge for review later\n",
    "\n",
    "merge_add[merge_add._merge == 'both']\n",
    "\n",
    "#Number of Values from merge\n",
    "maValues = merge_add.SHAPE.count().tolist()"
   ]
  },
  {
   "source": [
    "Use gdbJoin after editing and joining the address and sales tables."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "gdbJoin = locGDB / f'Par_Add_{dDate}'\n",
    "arc.SpatialJoin_analysis(f'{gdbPar}', f'{gdbAdd}', f'{gdbJoin}')\n",
    "df_join = features.GeoAccessor.from_featureclass(gdbJoin)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Columns from df_par not needed for merge\n",
    "# pkColumns = ['Name', 'SHAPE']\n",
    "# df_par.drop(columns=[x for x in df_par if x not in pkColumns], inplace=True)\n",
    "# df_par.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd075dad33b15f83a112a28040a32ba03061e24a62911504b8f236300170e604621",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}