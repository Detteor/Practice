{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Addresses for Parcels from Patriot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules and declare globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Use arcNew environment with this notebook.\n",
    "#Current Location: C:\\ProgramData\\Anaconda3\\envs\\arcNew\\python.exe\n",
    "import arcpy as arc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from shutil import copy2\n",
    "from pathlib import Path\n",
    "from arcgis import features\n",
    "from datetime import datetime as dt\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Folders in the Users Directory since it should have r/w permissions for the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "arc.env.overwriteOutput = True\n",
    "arc.env.outputZFlag = 'Disabled' #To remove z data from parcel fabric due to it being a polygonZ\n",
    "arc.env.outputMFlag = 'Disabled'\n",
    "arc.env.qualifiedFieldNames = False\n",
    "\n",
    "now = dt.now()\n",
    "mStr = now.strftime('%m%Y')\n",
    "dStr = now.strftime('%m_%d')\n",
    "uPath = Path.home()\n",
    "locFolders = ['Processing', 'Review']\n",
    "if uPath.exists():\n",
    "    for x in locFolders:\n",
    "        a = Path(uPath / 'GIS' / x)\n",
    "        if a.exists():\n",
    "            print(f'{a} already exists.')\n",
    "        else:\n",
    "            a.mkdir(parents=True)\n",
    "            print(f'{a} has been created.')\n",
    "else:\n",
    "    pass\n",
    "\n",
    "gisPath = uPath / 'GIS'\n",
    "lPath = [f for f in gisPath.glob('*')]\n",
    "netDir = Path(r'\\\\kcdp-1\\KCGIS\\MasterGISFiles\\Ben')\n",
    "netDB = netDir / 'GISPro' / 'SDE Connections'"
   ]
  },
  {
   "source": [
    "### Only use with notebook"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pd.options.display.max_columns = 40"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create File GeoDatabase and Feature Datasets"
   ]
  },
  {
   "source": [
    "### Create Folders for Parcels with Suffix Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create Folders for ParcelWithSuffix Data\n",
    "paFolder = [f for f in lPath if f.name == 'Processing'][0]\n",
    "paProcessing = paFolder / 'Patriot' / f'{dStr}'\n",
    "if paProcessing.exists() == True:\n",
    "    print(f'{paProcessing} already exist.')\n",
    "else:\n",
    "    paProcessing.mkdir(parents=True)\n",
    "    print(f'Created {paProcessing}.')\n",
    "\n",
    "paFR = [f for f in lPath if f.name == 'Review'][0]\n",
    "paReview = paFR / 'Patriot' / f'{dStr}'\n",
    "if paReview.exists() == True:\n",
    "    print(f'{paReview} already exist')\n",
    "else:\n",
    "    paReview.mkdir(parents=True)\n",
    "    print(f'Created {paReview}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iE = netDB / 'MAPPINGADMIN.sde' / 'PROD.MAPPINGADMIN.ParcelEditing'\n",
    "sr = arc.Describe(f'{iE}').spatialReference\n",
    "outGDB = gisPath / paFolder / f'Data_{mStr}.gdb'\n",
    "locGDB = outGDB / f'Daily_{dStr}'\n",
    "if arc.Exists(f'{outGDB}'):\n",
    "    print(\"GDB already exists.\")\n",
    "else:\n",
    "    arc.CreateFileGDB_management(f'{paFolder}', f'{outGDB.name}')\n",
    "    print(f'Created File GeoDatabase at {outGDB.parent}')\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "if arc.Exists(f'{locGDB}'):\n",
    "    print(f'{locGDB.name} already exists')\n",
    "else:\n",
    "    arc.CreateFeatureDataset_management(f'{locGDB.parent}', f'{locGDB.name}', sr)\n",
    "    print(f'{locGDB.name} Dataset has been created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Tax Parcels and Addressing FC into LOCAL GDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Does not work with Parcel Fabric due to bug with 10.5.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbAdd = 'PROD.ADDRESSINGADMIN.Addressing'\n",
    "dbSufP = 'PROD.GISADMIN.Suffix_Parcels'\n",
    "dbMPTab = 'PROD.gisadmin.MGOPermits'\n",
    "dbAPTab = 'PROD.gisadmin.All_Permits'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#create paths to the data instead of using env.workspace makes it so that I can use arc Walk\n",
    "arc.env.workspace = f'{iE.parent}'\n",
    "\n",
    "fcList = [dbAdd, dbSufP]\n",
    "tList = [dbMPTab, dbAPTab]\n",
    "cParcels = []\n",
    "cTables = []\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in fcList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}'\n",
    "            if arc.Exists(f'{locGDB / b}'):\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.FeatureClassToFeatureClass_conversion(f, f'{locGDB}', f'{b}')\n",
    "                cParcels.append(locGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "for dirpath, dirnames, filenames in arc.da.Walk():\n",
    "    for f in filenames:\n",
    "        if f in tList:\n",
    "            a  = f.split('.')[-1]\n",
    "            b = f'{a}_{dStr}' #Tables need to have a date string since they won't be under the dataset\n",
    "            # c = locGDB.parent\n",
    "            if arc.Exists(f'{outGDB / b}'):\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print(f'{b} already exists')\n",
    "            else:\n",
    "                arc.TableToTable_conversion(f, f'{outGDB}', f'{b}')\n",
    "                cTables.append(outGDB.joinpath(b))\n",
    "                print (f'{b} has been copied')\n",
    "# print(cParcels)\n",
    "# print(cTables)\n",
    "\n",
    "# Copying Suffix_Parcels\n",
    "# cList = ['PROD.GISADMIN.Suffix_Parcels']\n",
    "# iEPath = list(map(lambda x: iE.parent / x, cList))\n",
    "# print(f'{iEPath[0]}')\n",
    "\n",
    "# for f in iEPath:\n",
    "#     print(f'Copying FCs to {locGDB}')\n",
    "#     a = str(f).split('.')[-1]\n",
    "#     b = f'{a}_{dStr}'\n",
    "#     print(a)\n",
    "#     g = f'{f}'\n",
    "#     print(g)\n",
    "#     arc.FeatureClassToFeatureClass_conversion(g, f'{locGDB}',b)\n",
    "#     cParcels.append(locGDB / b)\n",
    "#     print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works with Parcel Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#currently using env.workspace to move parcelfabric needs to be changed to use path\n",
    "pfParcels = 'PROD.MAPPINGADMIN.ParcelFabric_Parcels'\n",
    "exp = \"TYPE = 7 AND Historical = 0\"\n",
    "a = pfParcels.split('.')[-1]\n",
    "b = f'{a}_{dStr}'\n",
    "# print(f'{iEPath[1]}')\n",
    "if arc.Exists(f'{locGDB / b}'):\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'{b} already exists')\n",
    "else:\n",
    "    print(f'Copying Parcel Fabric to {locGDB}')\n",
    "    arc.FeatureClassToFeatureClass_conversion(pfParcels, f'{locGDB}', b, where_clause=exp)\n",
    "    cParcels.append(locGDB.joinpath(b))\n",
    "    print(f'Finished Copying {b} to {locGDB}')"
   ]
  },
  {
   "source": [
    "### Import Excel Sheet into DataFrame"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pExcel = paProcessing / 'commercial properties.xlsx'\n",
    "df = pd.read_excel(pExcel)"
   ]
  },
  {
   "source": [
    "### Merge the Commerical Properties with the Parcel Fabric layer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Rename the columns to remove whitespace and change Parcel ID to match GIS Parcel #"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df.rename(columns=lambda x: ''.join(x.split()), inplace=True)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(field):\n",
    "    x = field.split(' ')\n",
    "    return f'{x[0]}-{x[1]}-{x[2]}-{x[3]}-{x[4]}-{x[5]}01'\n",
    "df.ParcelID = df.ParcelID.apply(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[df.ParcelID.str.contains('-00001')].shape)\n",
    "print(df.loc[~df.ParcelID.str.contains('-00001')].shape)"
   ]
  },
  {
   "source": [
    "Create DataFrame from Parcel Fabric Polygons"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "ppf_df = features.GeoAccessor.from_featureclass(pfFC, fields=['Name'])\n",
    "print('Parcel Fabric DataFrame created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppf_df.rename(columns={'Name': 'ParcelID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mDf = ppf_df.merge(df, on='ParcelID', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bDf = mDf[mDf._merge == 'both'].copy(deep=True)\n",
    "bDf.PropertyID = bDf.PropertyID.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bCsv = paReview / 'Both.csv'\n",
    "bDf.to_csv(bCsv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mCsv = paReview / 'Missing_Parcels.csv'\n",
    "mDf[(mDf._merge == 'right_only') & (mDf.ParcelID.str.contains('-00001'))].to_csv(mCsv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rCsv = paReview / 'Suffix_Parcels.csv'\n",
    "rDf = mDf[(mDf._merge == 'right_only') & (~mDf.ParcelID.str.contains('-00001'))].to_csv(rCsv, index=False)"
   ]
  },
  {
   "source": [
    "### Create DataFrame from Addressing Feature Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create Addressing DF from Addressing Feature Class\n",
    "addFC = [f for f in cParcels if f.name.startswith('Addressing') == True][0]\n",
    "df_add = features.GeoAccessor.from_featureclass(addFC)"
   ]
  },
  {
   "source": [
    "<b>Edit Address DataFrame to remove nulls, correct misnamed Roads, and to remove whitespace</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Create new address field using the MSAG columns."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add['STS'].replace('None', \" \")\n",
    "# df_add.loc[df_add.STS == None, :]\n",
    "# df_add.isnull().sum()\n",
    "df_add['STS'].fillna(np.nan, inplace= True)\n",
    "df_add['STS'].replace(' ', '', inplace=True)\n",
    "df_add['STS'].replace(np.nan, '', inplace=True)\n",
    "# df_add['STS'].unique()\n",
    "# df_add.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aType = df_add.STS.unique()\n",
    "# print(sType)\n",
    "for d in aType:\n",
    "    if d.upper() == 'ROAD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'RD').strip())\n",
    "    elif d.upper() == 'DRIVE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'DR').strip())\n",
    "    elif d.upper() == 'BOULEVARD':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'BLVD').strip())\n",
    "    elif d.upper() == 'LANE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'LN').strip())\n",
    "    elif d.upper() == 'COURT':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CT').strip())\n",
    "    elif d.upper() == 'CIRCLE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'CIR').strip())\n",
    "    elif d.upper() == 'STREET':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'ST').strip())\n",
    "    elif d.upper() == 'HIGHWAY':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'HWY').strip())\n",
    "    elif d.upper() == 'AVENUE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'AVE').strip())\n",
    "    elif d.upper() == 'PLACE':\n",
    "        df_add.STS = df_add.STS.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "        # sales_df['StreetType'] = sales_df.StreetType.apply(lambda x: x.replace(d, 'PL').strip())\n",
    "    else:\n",
    "        df_add.STS.str.strip()\n",
    "# print(df_add.STS.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.fillna(np.nan, inplace= True)\n",
    "# df_add.replace(' ', '', inplace=True)\n",
    "df_add.replace(np.nan, '', inplace=True)\n",
    "# print(df_add.columns)\n",
    "# print(df_add.STS.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.SAN = df_add.SAN.astype('str',errors='ignore')\n",
    "df_add.SAN = df_add.SAN.apply(lambda x: x.split('.')[0])\n",
    "df_add['FAddress'] = df_add[['SAN', 'PRD', 'STN', 'STS', 'POD']].values.tolist()\n",
    "# df_add.FAddress.values\n",
    "df_add['FooAddress'] = df_add['FAddress'].apply(' '.join)\n",
    "df_add['FoAddress'] = df_add['FooAddress'].apply(lambda x: ' '.join(x.split()))\n",
    "# df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add.FAddress = df_add.FoAddress\n",
    "kColumns = ['FAddress', 'SHAPE']\n",
    "# print([x for x in kColumns if x not in kColumns])\n",
    "df_add.drop(columns=[x for x in df_add if x not in kColumns], inplace=True)\n",
    "# df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_add_a = df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppf_df[ppf_df.ParcelID == '7-02-08520-01-0900-00001']"
   ]
  },
  {
   "source": [
    "bDf[bDf.SHAPE.isnull()]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfsr = {'rings': [[[0,0]]], 'spatialReference': {'wkid': 26957, 'latestWkid': 26957}}\n",
    "bDf.SHAPE = bDf.SHAPE.apply(lambda x: pfsr if x is None else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_join = bDf.spatial.join(df_add_a)\n",
    "add_join"
   ]
  },
  {
   "source": [
    "### Create Points from Parcel Suffix Feature Class and create DataFrame from the points FC."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Points out of polygons in Suffix_Parcels\n",
    "sufPar = [f for f in cParcels if f.name.startswith(\"Suffix\")][0]\n",
    "# print(sufPar.parent)\n",
    "# print(cParcels)\n",
    "sPoints = locGDB / 'sPoints'\n",
    "# print(sPoints)\n",
    "if arc.Exists(f'{sPoints}'):\n",
    "    print(f'{sPoints.name} already exists')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(sufPar), f'{sPoints}', \"INSIDE\")\n",
    "    print(f'{sPoints.name} has been created in {locGDB}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DF from Suffix_Parcels\n",
    "# sFields = [f.name for f in arc.ListFields(sPoints)]\n",
    "# print(sFields)\n",
    "sp_df = features.GeoAccessor.from_featureclass(sPoints)\n",
    "sp_df.drop(columns=['OBJECTID', 'ORIG_FID'], inplace=True)\n",
    "sp_df.rename(columns={'Name': 'Development', 'Lot':'DevLot'}, inplace=True)\n",
    "sp_df.rename(columns=lambda x: x.strip().upper(), inplace=True)\n",
    "print('Suffix Parcels DataFrame created.')"
   ]
  },
  {
   "source": [
    "### Merge Pride Tables (Pride Main and AS400 Parcel Data) with Parcels with Suffix Data(sp_df). Export them for Review. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<b> Left_Only = Values in map_df_a and right_only = Values in sp_df. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Pride Table (PrideMain and AS400 Data) and Parcels with Suffix Data (sp_df).\n",
    "df_Pride_m = map_df_a.merge(sp_df, on='PARCELID', how='outer', indicator=True)\n",
    "print(\"Created df_Pride_m from Pride Table and Addressing Data.\")\n",
    "# df_Pride_m.shape\n",
    "\n",
    "#export the csvs with both being the base data\n",
    "df_Pride_m[df_Pride_m['_merge'] == 'left_only'].to_csv(psReview / 'Pride_Left.csv', index=False)\n",
    "df_Pride_m[df_Pride_m['_merge'] == 'right_only'].to_csv(psReview / 'SP_Right.csv', index=False)\n",
    "df_Pride_m[df_Pride_m['_merge'] == 'both'].to_csv(psReview / 'Pride_Suf_Both.csv', index=False)\n",
    "print('Exported Pride Table and Suffix Data')"
   ]
  },
  {
   "source": [
    "df_Pride_mb is values that matched in the merge betwee Pride Table and Parcels With Suffix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_mb = df_Pride_m[df_Pride_m._merge == 'both'].copy(deep=True)\n",
    "p_suf_b = len(df_Pride_mb)\n",
    "# print(p_suf_b)"
   ]
  },
  {
   "source": [
    "### Take values not matched from df_Pride_m, which are from original Pride Main and AS400 Parcel Data merge, and try to match them with their address to df_add_a. Export for Review."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df_Pride_L = Values from df_Pride_m(merge between Pride Table and Suffix Parcels) that didn't match."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_L = df_Pride_m[df_Pride_m._merge == 'left_only'].copy(deep=True)\n",
    "p_suf_l = len(df_Pride_L)\n",
    "# print(p_suf_l)"
   ]
  },
  {
   "source": [
    "Remove Unit from the FLocation Column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc(field):\n",
    "    a = field.split(', ')\n",
    "    if a[-1].lower().startswith('unit'):\n",
    "        return a.pop(0)\n",
    "    else:\n",
    "        return field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_L.FLOCATION = df_Pride_L.FLOCATION.apply(loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove _merge column\n",
    "# del df_Pride_m['_merge']\n",
    "#Unmatched Values from Pride Table (Pride and AS400 Data) merged with Addressing Data\n",
    "df_Pride_L.drop(columns=['_merge', 'SHAPE'] , inplace=True)\n",
    "df_Pride_all = df_Pride_L.merge(df_add_a, left_on='FLOCATION', right_on='FAddress', how='outer', indicator=True)\n",
    "# print(df_Pride_all.shape)\n",
    "print('Created df_Pride_all from Pride Table and Addressing Data.')"
   ]
  },
  {
   "source": [
    "<b> Left_Only = Values in df_Pride_all. Right_Only = Values in df_Pride_L."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csvs to see what needs to be fixed both needs to be appended to df_Pride_m\n",
    "df_Pride_all[df_Pride_all['_merge'] == 'left_only'].to_csv(psReview / 'ASP_Add_Left.csv', index=False)\n",
    "df_Pride_all[df_Pride_all['_merge'] == 'both'].to_csv(psReview / 'ASP_Add_Both.csv', index=False)\n",
    "df_Pride_all[df_Pride_all['_merge'] == 'right_only'].to_csv(psReview / 'ASP_Add_Right.csv', index=False)\n",
    "print('Exported Pride Table and Address Table.')"
   ]
  },
  {
   "source": [
    "<b> df_Pride_ab is values that matched between Pride, Suffix Merge and Addressing Data. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_ab = df_Pride_all[df_Pride_all._merge == 'both'].copy(deep=True)\n",
    "p_add_b = len(df_Pride_ab)\n",
    "# print(p_add_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_al = df_Pride_all[df_Pride_all._merge == 'left_only'].copy(deep=True)\n",
    "p_add_l = len(df_Pride_al)\n",
    "# print(p_add_l)"
   ]
  },
  {
   "source": [
    "loc_df are values that don't start with a number. geo_df are values that start with a number to make it easier to geocode."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = df_Pride_al[~df_Pride_al.FLOCATION.str.contains('^[0-9]+')].copy(deep=True)\n",
    "geo_df = df_Pride_al[df_Pride_al.FLOCATION.str.contains('^[0-9]+')].copy(deep=True)"
   ]
  },
  {
   "source": [
    "<b> Added PARENT Column to match with Parcel Fabric DataFrame, edited addressing to match addressing DataFrame, updated the columns names, created column (OriginalAdd) to maintain the non-edited address. </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(field):\n",
    "    a = field.split('-')\n",
    "    return f'{a[0]}-{a[1]}-{a[2]}-{a[3]}-{a[4]}-00001'\n",
    "\n",
    "loc_df['PARENT']= loc_df.PARCELID.apply(change)\n",
    "geo_df['PARENT'] = geo_df.PARCELID.apply(change)"
   ]
  },
  {
   "source": [
    "Clear up column names in loc_df and geo_df"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.dropna(axis=1, how='all', inplace=True)\n",
    "loc_df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df.dropna(axis=1, how='all', inplace=True)\n",
    "geo_df.drop(columns='_merge', inplace=True)"
   ]
  },
  {
   "source": [
    "### CSV with manual edits to get address data into correct format. Created from ASP_Add_Left output. Do not use for automation since the parcels could change in the future."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create df from edits CSV\n",
    "pDir = netDir / 'ParcelWithSuffix' / 'Edits_ASP_Add_Left.csv'\n",
    "cLocal = psReview / pDir.name\n",
    "print(f'Copying {pDir.name} to {cLocal.parent}')\n",
    "copy2(pDir, cLocal)\n",
    "print(f'Finished Copying {cLocal.name} to {cLocal.parent}')\n",
    "df_Pride_edits = pd.read_csv(cLocal)\n",
    "df_Pride_edits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_edits.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_edits = df_Pride_edits.rename(columns={'Unnamed: 7' : 'OriginalAdd'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_add_a.columns)\n",
    "print(df_Pride_edits.columns)"
   ]
  },
  {
   "source": [
    "<b> Merge Addressing with edits </b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge addressing with df_Pride_edits\n",
    "df_Pride_Add = df_Pride_edits.merge(df_add_a, left_on='UPPER_ADDR', right_on='Address', how='outer', indicator=True)\n",
    "# df_Pride_Add"
   ]
  },
  {
   "source": [
    "Left_Only = Values from df_Pride_edits. Right_Only = Values from df_add"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_Add[df_Pride_Add['_merge'] == 'left_only'].to_csv(psReview / 'Ed_Add_Left.csv', index=False)\n",
    "df_Pride_Add[df_Pride_Add['_merge'] == 'both'].to_csv(psReview / 'Ed_Add_Both.csv', index=False)\n",
    "df_Pride_Add[df_Pride_Add['_merge'] == 'right_only'].to_csv(psReview / 'Ed_Add_Right.csv', index=False)\n",
    "print('Exported Pride Table Edits and Address Table.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_Add.loc[df_Pride_Add._merge == 'left_only']\n",
    "#  & df_Pride_Add.FullAddr.str.contains('1679')\n",
    "df_Pride_AddL = df_Pride_Add.loc[df_Pride_Add._merge == 'left_only']\n",
    "df_Pride_AddB = df_Pride_Add.loc[df_Pride_Add._merge == 'both']\n",
    "print(df_Pride_AddL.shape)\n",
    "print(df_Pride_AddB.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_add.loc[(df_add.STN.str.contains('KENTWOOD', na=False)) & (df_add['SAN'].astype(str).str.contains('23', na=False))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Pride_AddL.drop(columns='_merge', inplace=True)\n",
    "# df_Pride_AddL.drop(columns='SHAPE', inplace=True)\n",
    "df_Pride_AddL.columns"
   ]
  },
  {
   "source": [
    "### Create Points from Parcel Fabric for geospatial information for Addresses and Parcels with Suffixes that don't match"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to not rely on arc.env.workspace for outputs.\n",
    "pfFC = [f for f in cParcels if f.name.startswith('ParcelFabric') == True][0]\n",
    "# print(pfFC)\n",
    "pfPoints = locGDB / f'pfPoints_{dStr}'\n",
    "# print(pfPoints)\n",
    "if arc.Exists(f'{pfPoints}'):\n",
    "    print(f'{pfPoints.name} already exists.')\n",
    "else:\n",
    "    arc.FeatureToPoint_management(str(pfFC), f'{pfPoints}', \"INSIDE\")\n",
    "    print(f'{pfPoints.name} has been created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from pfPoints FC\n",
    "pfP_df = features.GeoAccessor.from_featureclass(pfPoints, fields=['Name'])\n",
    "print('Parcel Fabric Points DataFrame created.')\n",
    "# pfP_df.shape\n",
    "# pfP_df.columns\n",
    "\n",
    "#Use if need to edit field names in pfPoints FC\n",
    "# pfFields = [f.name for f in arc.ListFields(pfPoints)]\n",
    "# print(pfFields)"
   ]
  },
  {
   "source": [
    "<b> Attempt to geocode the final addresses. Can use OSM Nominatim, ESRI AGOL (uses credits from org), FirstMap Locator on their REST Server, or KC batch geocoder on REST Server.</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del Add_Par_df['_merge']\n",
    "# Add_Par_df = df_Pride_AddL.merge(pfP_df, how='inner', left_on='Fu', right_on='Name')\n",
    "# Add_Par_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add_Par_df[Add_Par_df.Name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_Pride_AddL[df_Pride_AddL.Fu.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addPardf = pReview / 'AddParDF.csv'\n",
    "# Add_Par_df.to_csv(addPardf, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from arcgis.gis.server import Server\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "pURL = os.getenv('PORTAL_SITE')\n",
    "aURL = os.getenv('AGOL_SITE')\n",
    "sURL = os.getenv('STAGE_SITE')\n",
    "sUser = os.getenv('STAGE_USERNAME')\n",
    "sPass = os.getenv('STAGE_PASSWORD')\n",
    "aUser = os.getenv('AGOL_USERNAME')\n",
    "aPass = os.getenv('AGOL_PASSWORD')\n",
    "pUser = os.getenv('ESRI_USERNAME')\n",
    "pPass = os.getenv('ESRI_PASSWORD')\n",
    "\n",
    "#Connect to Enterprise GIS\n",
    "gisE = GIS(url=pURL, username=pUser, password=pPass)\n",
    "\n",
    "#Connect to AGOL\n",
    "gisA = GIS(url=aURL, username=aUser, password=aPass, set_active=False)\n",
    "\n",
    "#Connect to Staging. Does not work for right now. Found a solution: https://community.esri.com/t5/developers-questions/error-connecting-to-arcgis-server-with-arcgis-python-api/m-p/869042?commentID=896496#comment-896496\n",
    "\n",
    "# gisT = GIS(url=sURL, username=sUser, password=sPass, set_active=False, verify_cert=False)\n",
    "# from arcgis.gis.server import Server\n",
    "# sURL = 'https://ESRISTAGE:6443'\n",
    "# gisT = Server(url=f'{sURL}/ed8fb7ce-e99d-4eb2-af78-b7ef11e3c2c4/admin',token_url=f'{sURL}/ed8fb7ce-e99d-4eb2-af78-b7ef11e3c2c4/tokens/generateToken', username=sUser, password=sPass, verify_cert=False, set_active=False)\n",
    "\n",
    "#This method for connecting to staging server does work. Must use Server Module for now\n",
    "gisT = Server(url=f'{sURL}/arcgis/admin', username=sUser, password=sPass, verify_cert=False)\n",
    "\n",
    "# gisT = GIS(url=f'{sURL}/arcgis/admin', username=sUser, password=sPass, verify_cert=False)\n",
    "\n",
    "\n",
    "print(gisA)\n",
    "print(gisE)\n",
    "print(gisT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from arcgis.gis.server import ServerManager\n",
    "\n",
    "# gisT.datastores.add_database()\n",
    "\n",
    "# kServ = gisE.admin.servers.list()[0]\n",
    "# kServ.datastores.search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addz(field, field1):\n",
    "    if field1 != ' ':\n",
    "        return f'{field}, {field1}'\n",
    "    else:\n",
    "        return field\n",
    "geo_df['ZLOCATION'] = geo_df.apply(lambda x: addz(x['FLOCATION'], x['ZIPCODE']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Geocoding from_df is very buggy, changed the _accessor.py located at C:\\Users\\MKinnaman\\Anaconda3\\envs\\arcNew\n",
    "#\\Lib\\site-packages\\arcgis\\features\\geo. Added Line 2247 to 2250 and Line 2261.\n",
    "\n",
    "from arcgis import geocoding\n",
    "from arcgis.geocoding import Geocoder\n",
    "# from arcgis.features import GeoAccessor\n",
    "\n",
    "locFM = 'https://enterprise.firstmap.delaware.gov/arcgis/rest/services/Location/Delaware_FirstMap_Locator/GeocodeServer'\n",
    "geoc = Geocoder(locFM)\n",
    "\n",
    "# print(geoc.properties.locatorProperties.MaxBatchSize)\n",
    "\n",
    "# def addST(field):\n",
    "#     return field + ', DOVER, DE'\n",
    "# df_Pride_AddL['GAddress'] = df_Pride_AddL.UPPER_ADDR.apply(addST)\n",
    "# df_Pride_AddL\n",
    "\n",
    "\n",
    "# if len(geo_df.ZLOCATION) <= 499:\n",
    "#     aList = geo_df.ZLOCATION.values.tolist()\n",
    "# else:\n",
    "#     clist = [aList[i:i + 499] for i in range(0, len(aList), 499)]\n",
    "\n",
    "# add_geom = []\n",
    "# geo_b = {}\n",
    "# if aList is not None:\n",
    "#     geo = geocoding.batch_geocode(aList, out_sr=26957, geocoder= geoc)\n",
    "#     add_geom.append(geo)\n",
    "# else:\n",
    "#     j = len(clist)\n",
    "#     for i in range(0, j):\n",
    "#       geo = geocoding.batch_geocode(clist[i], )\n",
    "\n",
    "geo_B = features.GeoAccessor.from_df(geo_df, address_column='ZLOCATION', geocoder=geoc, sr=26957)\n",
    "geo_B"
   ]
  },
  {
   "source": [
    "Create CSV of geocoded addresses."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_add = psReview / 'Geo_Add.csv'\n",
    "geo_add_df = geo_B[geo_B.SHAPE.notnull()].copy(deep=True)\n",
    "geo_add_df.to_csv(geo_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_match = len(geo_add_df)\n",
    "# print(geo_match)"
   ]
  },
  {
   "source": [
    "Get the values that were not matched after geocoding and merge them with the Parcel ID from Parcel Fabric Points (pfP_df)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_b_null = geo_B[geo_B.SHAPE.isnull()].copy(deep=True)\n",
    "del geo_b_null['SHAPE']\n",
    "geo_pf = geo_b_null.merge(pfP_df, left_on='PARENT', right_on='Name', how='outer', indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = psReview / 'MissingParcels.csv'\n",
    "geo_pf.loc[geo_pf._merge == 'left_only', :].to_csv(dl, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_pf_df = geo_pf[geo_pf._merge == 'both'].copy(deep=True)\n",
    "geo_pf_b = len(geo_pf_df)\n",
    "# print(geo_pf_b)"
   ]
  },
  {
   "source": [
    "Merge parcels with an address with Parcel Fabric Points."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "pl = psReview / 'MissingParcels_2.csv'\n",
    "loc_pf = loc_df.merge(pfP_df, left_on='PARENT', right_on='Name', how='outer', indicator=True)\n",
    "loc_pf.loc[loc_pf._merge == 'left_only', :].to_csv(pl, index=False)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_pf_df = loc_pf.loc[loc_pf._merge == 'both', :]\n",
    "loc_pf_b = len(loc_pf_df)\n",
    "# print(loc_pf_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [p_suf_b, p_add_b, geo_pf_b, loc_pf_b, geo_match]\n",
    "sum(ls)"
   ]
  },
  {
   "source": [
    "<b>Concat all the DFs into one. All of the DFs need to have the same columns. df_Pride_mb, df_pride_ab, geo_pf_df, loc_pf_df, and geo_add_df</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geo_pf_df.columns)\n",
    "print(df_Pride_ab.columns)\n",
    "print(df_Pride_mb.columns)\n",
    "print(loc_pf_df.columns)\n",
    "print(geo_add_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_pf_df.drop(columns=['_merge','Name'], inplace=True)\n",
    "gList = geo_pf_df.columns.tolist()\n",
    "# gList.remove('_merge')\n",
    "# kCol = ['PARCELID']\n",
    "# print(aClist)\n",
    "# df_Pride_ab.drop(columns=[x for x not in gList], inplace=True)\n",
    "# map_df_a.columns\n",
    "df_Pride_ab.drop(columns=[x for x in df_Pride_ab.columns if x not in gList], inplace=True)\n",
    "df_Pride_mb.drop(columns=[x for x in df_Pride_mb.columns if x not in gList], inplace=True)\n",
    "loc_pf_df.drop(columns=[x for x in loc_pf_df.columns if x not in gList], inplace=True)\n",
    "geo_add_df.drop(columns=[x for x in geo_add_df.columns if x not in gList], inplace=True)"
   ]
  },
  {
   "source": [
    "Add PARENT column to all of the DFs for use with MGO Model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pride_ab['PARENT'] = df_Pride_ab.PARCELID.apply(change)\n",
    "df_Pride_mb['PARENT'] = df_Pride_mb.PARCELID.apply(change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [geo_pf_df, df_Pride_ab, df_Pride_mb, loc_pf_df, geo_add_df]\n",
    "df_Append_all = pd.concat(frames, axis=0)\n",
    "cDF = psReview / 'ParcelWithSuffix.csv'\n",
    "df_Append_all.to_csv(cDF, index=False)"
   ]
  },
  {
   "source": [
    "Edit the column names for clarity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oCols = [f.lower() for f in df_Append_all.columns]\n",
    "rCols = {'PARCELID': 'Name', 'FLOCATION' : 'Address'}\n",
    "dCols = ['BILLINGADDRESS', 'BILLINGADDRESS2', 'ZIPCODE', 'FORMATTEDLOCATION', 'ZLOCATION']\n",
    "df_Append_all.drop(columns=dCols, inplace=True)\n",
    "df_Append_all.rename(columns=rCols, inplace=True)\n",
    "# df_Append_all.rename(columns=lambda x: x.title(), inplace=True)\n",
    "# df_Append_all.rename(columns=lambda x: x.upper(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Append_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Append_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "source": [
    "Testing for bugs in arcgis package"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outc = psReview / 'all.csv'\n",
    "# df_Append_all.to_csv(outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(geo_pf_df.spatial.sr)\n",
    "# print(df_Pride_ab.spatial.sr)\n",
    "# print(df_Pride_mb.spatial.sr)\n",
    "# print(loc_pf_df.spatial.sr)\n",
    "# print(geo_add_df.spatial.sr)\n",
    "# print(df_Append_all.spatial.sr)\n",
    "# print(pfP_df.spatial.sr)\n",
    "# print(geo_B.spatial.sr)\n",
    "# outFC = locGDB / f'geo_Test'\n",
    "# geo_pf_df.spatial.to_featureclass(f'{outFC}', sanitize_columns=False)\n",
    "# from uuid import uuid4\n",
    "# import random\n",
    "# import string\n",
    "# for x in frames:\n",
    "#     outFC = locGDB / f'{random.choice(string.ascii_lowercase)}'\n",
    "#     a = pd.DataFrame(x)\n",
    "#     a.spatial.to_featureclass(location=f'{outFC}', sanitize_columns=False)\n",
    "\n",
    "# frames = [geo_pf_df, df_Pride_ab, df_Pride_mb, loc_pf_df, geo_add_df]"
   ]
  },
  {
   "source": [
    "Output the DataFrame to the geodatabase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFC = locGDB / f'ParcelWithSuffix_{dStr}'\n",
    "df_Append_all.spatial.to_featureclass(f'{outFC}', sanitize_columns=False)\n",
    "print(f'Copied {outFC.name} to {locGDB}')"
   ]
  },
  {
   "source": [
    "Publish the DataFrame to Portal for use with online mapping."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Append_all.spatial.to_featurelayer('ParcelWithSuffix', gis= gisE)\n",
    "print('Added ParcelWithSuffix to Portal')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3710jvsc74a57bd0be568a45c5be0d4caf76dff1c2f5b3b93950ef2ed99d2899e809934a8a7c9d46",
   "display_name": "Python 3.7.10 64-bit ('arcNew': conda)"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}